<!DOCTYPE html>
<html>
<font face="Verdana" size = "2.5">
<head>

<style type="text/css">
A:link { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:visited { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:active { COLOR: #0903A5; TEXT-DECORATION: none }
A:hover { COLOR: #C80537; TEXT-DECORATION: none; font-weight: none }
img {
  margin-top: -20px;
  margin-bottom: 0px;
}
h1 {
  margin-top: 15px;
  margin-bottom: 15px;
}
h2 {
  margin-top: 10px;
  margin-bottom: 10px;
}
h2 + ul {
  margin-top: -5px;
  margin-bottom: 0px;
}
h2 + ol {
  margin-top: 0px;
  margin-bottom: 20px;
}
h3 + ol {
  margin-top: -10px;
  margin-bottom: -10px;
}
hr {
    display: block;
    height: 1px;
    border: 0;
    border-top: 1px solid #ccc;
    margin: 1em 0;
    padding: 0;
}
</style>

<title>Paul Liang, CMU</title>

</head>

<body>

<img src="images/photo2023_small.jpeg" width="170" height="170" style="float: left; PADDING-TOP: 20px; PADDING-RIGHT: 10px; PADDING-BOTTOM: 0px"/>

<h1>Paul Pu Liang</h1>
Email: ppliang(at)mit.edu<br/>
Assistant Professor, <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a><br/>
<br/>

<a href="papers/cv_05_2024.pdf"><font size="+1.5">[CV]</font></a>

<a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ"> <img src="images/gscholar.jpg" width="24" height="24" style="margin-bottom:-7px;PADDING-LEFT:5px;PADDING-RIGHT:5px">[google scholar]</a>

<a href="https://www.linkedin.com/in/paulpuliang/"> <img src="images/linkedin.png" width="20" height="20" style="margin-bottom:-5px;PADDING-LEFT:5px;PADDING-RIGHT:5px">[linkedin]</a> 

<a href="https://github.com/pliang279"> <img src="images/github.png" width="30" height="30" style="margin-bottom:-10px">[github]</a>

<a href="https://twitter.com/pliang279/"> <img src="images/twitter.png" width="30" height="30" style="margin-bottom:-10px">[twitter]</a>

<a href="https://www.instagram.com/lpwinniethepu/"> <img src="images/instagram.png" width="22" height="22" style="margin-bottom:-6px;PADDING-LEFT:5px;PADDING-TOP: 10px"> [instagram]</a>

<br/>
<hr/>

<a href="https://pliang279.github.io/papers/"><font size="+1.5">[Papers]</font></a> <a href="#Teaching"><font size="+1.5">[Teaching]</font></a> <a href="#Group"><font size="+1.5">[Research Group]</font></a> <a href="#Talks"><font size="+1.5">[Talks]</font></a> 

<br/>
<br/>
<font color="#DC143C">I will be joining MIT as an Assistant Professor in Fall 2024, joint between the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>.</font><br/>
I will direct the new multisensory intelligence research group, with a theme of developing multisensory AI for the human experience. My group has two thrusts:<br/>
(1) Foundations of multisensory AI that can learn and interact with the world through integrating diverse sensory channels such as text, speech, audio, video, physical sensors, and physiological messages.<br/>
(2) Impact on the human experience: AI for human physical, emotional, and social well-being, multimedia generative AI to augment human creativity, climate and environment sensing, and AI for human sensory experiences like music, art, cultures, smell, and taste.
<br/>
<br/>
I recently received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
I was also fortunate to collaborate with <a href="https://www.cs.cmu.edu/~mblum/">Manuel Blum</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/lblum.html">Lenore Blum</a>, <a href="https://faisal.ai/">Faisal Mahmood</a>, <a href="https://jmhessel.com/">Jack Hessel</a>, and <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a> at Berkeley, Harvard Medical School, and UW/AI2.
My research was generously supported by a <a href="https://www.siebelscholars.com/scholar-profile/3723/">Siebel Scholars Award</a>, Waibel Presidential Fellowship, <a href="https://research.fb.com/fellows/liang-paul-pu/">Facebook PhD Fellowship</a>, and <a href="https://www.cs.cmu.edu/cmlh/digital-health-archive/cmlh-digital-health-fellows-2021">Center for Machine Learning and Health Fellowship</a>,
and has been recognized by 4 best paper/honorable mention awards at international conferences and workshops.
I love teaching and was honored to receive the <a href="https://www.cs.cmu.edu/~scsfacts/perlis.html">Alan J. Perlis Graduate Student Teaching Award</a> for co-instructing courses on multimodal machine learning.
Previously, I received an M.S. in Machine Learning and a B.S. with University Honors in Computer Science and Neural Computation from CMU.
<br/>
<br/>
<b>Research opportunities:</b> I am happy to collaborate and answer questions about my research and MIT academic programs. If you are interested, please send me an email. I especially encourage students from underrepresented groups to reach out.
<br/>

<h2 id="News">News</h2>

<ul style="list-style-type:disc; line-height:140%">

  <li><font color="#DC143C">2023</font>: Excited to release some recent work formalizing and quantifying multimodal interactions from <a href="https://arxiv.org/abs/2302.12247/">statistical (NeurIPS 2023)</a> and <a href="https://arxiv.org/abs/2306.04125/">human (ICMI 2023)</a> perspectives, with applications in <a href="https://arxiv.org/abs/2207.00056/">visualizing and interpreting multimodal models (ICLR 2023)</a>, <a href="https://arxiv.org/abs/2306.05268/">contrastive learning of unique information (NeurIPS 2023)</a>, and <a href="https://arxiv.org/abs/2306.04539/">guarantees for multimodal semi-supervised learning (arXiv 2023)</a>.

  <li><font color="#DC143C">2023</font>: Co-teaching <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">11-777 Multimodal Machine Learning, Fall 2023</a>, course content will be updated on the website. 

  <li><font color="#DC143C">2023</font>: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on multimodal machine learning</a> at ICML 2023, ICMI 2023, CVPR 2022 and NAACL 2022, teaching at CIFAR DLRL summer school and African Masters of Machine Intelligence (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>): check out our <a href="https://arxiv.org/abs/2209.03430">survey paper</a>, <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">slides, and videos</a>.

  <li><font color="#DC143C">2023</font>: Nothing has excited me more than collaborating with and advising great students. I've learned so much from them and I'm excited to watch them embark on their new research agendas - follow their work for more exciting new ideas! <a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a> -> PhD at Princeton, <a href="https://rulinshao.github.io/">Rulin Shao</a> -> PhD at UW, <a href="https://xiangfan.io/">Xiang Fan</a> -> PhD at UW, <a href="https://jivatneet.github.io/">Jivat Neet</a> -> PhD at Berkeley, <a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a> -> PhD at Michigan, <a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a> -> PhD at MIT, <a href="https://peter.onrender.com/">Peter Wu</a> -> PhD at Berkeley, <a href="https://dongwonl.com/">Dong Won Lee</a> -> PhD at MIT, <a href="https://terranceliu.github.io/">Terrance Liu</a> -> PhD at CMU.

  <li><font color="#DC143C">2023</font>: LP and I are teaching 2 new graduate seminar courses: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">11-866 Artificial Social Intelligence</a> and <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">11-877 Advanced Topics in Multimodal Machine Learning</a>.

  <li><font color="#DC143C">2022</font>: Check out course content for <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">11-777 Multimodal Machine Learning, Fall 2022</a>, where LP and I have completely revamped the course content. Also check out fully recorded <a href="https://www.youtube.com/channel/UCqlHIJTGYhiwQpNuPU5e2gg">lecture videos</a> and <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2020/">course content</a> for 11-777 in Fall 2020.</li></li>

  <li><font color="#DC143C">2022</font>: Are you working on multimodal tasks and can't decide on a model? Check out <a href="https://arxiv.org/abs/2203.01311">HighMMT (TMLR 2022)</a>, our attempt at a single multimodal model that can predict sentiment, emotion, humor, disease, robot pose, and more, as well as <a href="https://arxiv.org/abs/2107.07502">MultiBench (NeurIPS 2021)</a> and <a href="https://arxiv.org/abs/2306.16413">MultiZoo (JMLR 2022)</a>, a large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.</li>

  <li><font color="#DC143C">2021</font>: Extremely honored to have received a <a href="https://research.facebook.com/fellows/liang-paul-pu/">Facebook PhD Fellowship</a> and a <a href="https://www.cs.cmu.edu/cmlh/digital-health-archive/cmlh-digital-health-fellows-2021">Center for Machine Learning and Health Fellowship</a> to support my research in socially intelligent AI! For students applying for graduate fellowships, I uploaded my <a href="papers/research_statement_paul_liang_2020.pdf">statement from the 2020 application</a>.</li>

  <li><font color="#DC143C">2020</font>: Check out the <a href="https://blog.ml.cmu.edu/">CMU Machine Learning Blog</a> - new research and educational content every few weeks on ML research going on at CMU!</li>
</ul>

<h2 id="Publications">Selected Publications</h2>

(* denotes joint first-authors, see <a href="https://pliang279.github.io/papers/">full list of publications here)</a></br>

<h3>Foundations of multimodal machine learning:</h3>

<ul style="line-height:140%">

  <li><b>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</b></li>
  Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICLR 2024<br/>
  <a href="https://arxiv.org/abs/2306.04539">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</b></li>
  Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2023<br/>
  <a href="https://arxiv.org/abs/2302.12247">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</b></li>
  Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  ACM Computing Surveys, Tutorials at ICML 2023, ICMI 2023, CVPR 2022, NAACL 2022<br/>
  <a href="https://arxiv.org/abs/2209.03430">[arXiv]</a> <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">[tutorial website]</a> <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">[tutorial videos]</a>
</ul>

<h3>Representation learning over multisensory and temporal data:</h3>

<ul style="line-height:140%">

  <li><b>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  TMLR 2022<br/>
  <a href="https://arxiv.org/abs/2203.01311">[arXiv]</a> <a href="https://github.com/pliang279/HighMMT">[code]</a>

  <li><b>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2021 and JMLR Open Source Software 2022<br/>
  <a href="https://arxiv.org/abs/2107.07502">[arXiv]</a> <a href="https://arxiv.org/abs/2306.16413">[software]</a> <a href="https://multibench.readthedocs.io/en/latest/">[website]</a> <a href="https://github.com/pliang279/MultiBench">[code]</a>

  <li><b>Multimodal Transformer for Unaligned Multimodal Language Sequences</b></li>
  Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ACL 2019<br/>
  <a href="https://arxiv.org/abs/1906.00295">[arXiv]</a> <a href="https://github.com/yaohungt/Multimodal-Transformer">[code]</a>
</ul>

<h3>Multimodal applications in social AI, health, and wellness:</h3>

<ul style="line-height:140%">

  <li><b>Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction</b></li>
  Guillaume Jaume, Anurag Vaidya, Richard Chen, Drew Williamson, Paul Pu Liang, Faisal Mahmood<br/>
  CVPR 2024<br/>
  <a href="https://arxiv.org/abs/2304.06819">[arXiv]</a> <a href="https://github.com/mahmoodlab/SurvPath">[code]</a>

  <li><b>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</b></li>
  Paul Pu Liang*, Terrance Liu*, Anna Cai, Michal Muszynski, Ryo Ishii, Nick Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2021 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2106.13213">[arXiv]</a>

  <li><b>Computational Modeling of Human Multimodal Language: The MOSEI Dataset and Interpretable Dynamic Fusion</b></li>
  Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  Master's Thesis, CMU Machine Learning Data Analysis Project 2018 <font color="#DC143C">(first runner-up award)</font><br/>
  <a href="papers/dap2018_mosei.pdf">[paper]</a> <a href="slides/dap2018_mosei_slides.pdf">[slides]</a> <a href="posters/dap2018_mosei_poster.pdf">[poster]</a>

  <li><b>Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning</b></li>
  Minghai Chen*, Sen Wang*, Paul Pu Liang*, Tadas Baltru&scaron;aitis, Amir Zadeh, Louis-Philippe Morency<br/>
  ICMI 2017 <font color="#DC143C">(oral, honorable mention award)</font><br/>
  <a href="https://arxiv.org/abs/1802.00924">[arXiv]</a> <a href="https://drive.google.com/drive/u/0/folders/1NxyFuogyzNFoCH0Zi5aIXUGYKGuSY9TY">[code]</a> <a href="slides/icmi2017_gme_slides.pdf">[slides]</a>
</ul>

<h3>Real-world societal concerns:</h3>

<ul style="line-height:140%">

  <li><b>Towards Understanding and Mitigating Social Biases in Language Models</b></li>
  Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICML 2021<br/>
  <a href="https://arxiv.org/abs/2106.13219">[arXiv]</a> <a href="https://github.com/pliang279/LM_bias">[code]</a>

  <li><b>Towards Debiasing Sentence Representations</b></li>
  Paul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2020<br/>
  <a href="https://arxiv.org/abs/2007.08100">[arXiv]</a> <a href="https://github.com/pliang279/sent_debias">[code]</a>

  <li><b>Think Locally, Act Globally: Federated Learning with Local and Global Representations</b></li>
  Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award)</font><br/>
  <a href="https://arxiv.org/abs/2001.01523">[arXiv]</a> <a href="https://github.com/pliang279/LG-FedAvg">[code]</a>
</ul>

<h2 id="Teaching">Teaching</h2>

<ul style="list-style-type:disc; line-height:140%">

  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2024, CMU with Daniel Fried <br/>
  <li>Co-Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">11-777 Multimodal Machine Learning</a>, Fall 2023, CMU with Louis-Philippe Morency <br/>
  <li>Instructor: Multimodal Artificial Intelligence (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>), African Masters Of Machine Intelligence, Summer 2023
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on Multimodal ML</a> at ICML 2023, ICMI 2023, CVPR 2022 and NAACL 2022 with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">11-866 Artificial Social Intelligence</a>, Spring 2023, CMU with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2023, CMU with Louis-Philippe Morency <br/>
  <li>Co-Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">11-777 Multimodal Machine Learning</a>, Fall 2022, CMU with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2022, CMU with Louis-Philippe Morency and Amir Zadeh <br/>
  <li>Guest Lecturer: <a href="https://deeplearning-cmu-10707-2022spring.github.io/">10-707 Deep Learning</a>, <a href="https://sites.google.com/andrew.cmu.edu/haii-cmu/">05-618 Human AI Interaction</a>, <a href="https://fall22.mayankgoel.courses/">17-728 Machine Learning and Sensing</a>, Peking University, University of Florida.<br/>
  Lectures on multimodal machine learning <a href="https://drive.google.com/file/d/1IkIRTQfIcp61qqW3dCYKgnpjbN0vRo-U/view">[slides]</a> <a href="https://www.youtube.com/watch?v=9v6Xg5Nk76M&t=3467s&ab_channel=PaulLiang">[video]</a>
  <li>Head TA & Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2020/">11-777 Multimodal Machine Learning</a>, Fall 2020, CMU. Instructor: Louis-Philippe Morency<br/>
  4 lectures on multimodal tasks <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kemfewi19ex6cc">[slides]</a> <a href="https://www.youtube.com/watch?v=fBYu8I52nVM">[video]</a>, deep generative models <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kglbikewtgy3xt">[slides]</a> <a href="https://www.youtube.com/watch?v=qEbYtPhG768">[video]</a>, reinforcement learning <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kgs9n4o8ft81wp">[slides]</a> <a href="https://www.youtube.com/watch?v=OI02F2XEe_0">[video]</a>, and multimodal RL <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kguil6snycmb">[slides]</a> <a href="https://www.youtube.com/watch?v=UsAgvMC5fRs">[video]</a>.<br/>
  Public videos on YouTube have amassed more than 10000 views.</li>
  <li>Head TA & Lecturer: <a href="https://piazza.com/cmu/fall2019/11777/resources">11-777 Multimodal Machine Learning</a>, Fall 2019, CMU. Instructor: Louis-Philippe Morency<br/>
  2 lectures on reinforcement learning <a href="https://piazza.com/class_profile/get_resource/jv5kp3m5nmu43l/k1tb5xj1jn92ca">[slides]</a> and multimodal RL <a href="https://piazza.com/class_profile/get_resource/jv5kp3m5nmu43l/k1vh2akqdht80">[slides]</a></li>
  <li>TA: <a href="https://sailinglab.github.io/pgm-spring-2019/">10-708 Probabilistic Graphical Models</a>, Spring 2019, CMU. Instructor: Eric Xing</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~10715-f18/">10-715 Advanced Introduction to Machine Learning</a>, Fall 2018, CMU. Instructor: Maria-Florina Balcan</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~roni/10601/">10-601 Introduction to Machine Learning</a>, Fall 2016, CMU. Instructor: Roni Rosenfeld</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~213/">15-213/18-213/15-513 Introduction to Computer Systems</a>, Summer 2016, CMU. Instructor: Brian Railing</li>
</ul>

<h2 id="Group">Research Group</h2>

Some amazing students I've had the pleasure of advising:
<br/>

<ul style="list-style-type:disc; line-height:140%">
  
  <li><a href="https://rpandey.tech/">Rohan Pandey</a>, now at Reworkd AI (YC S23) (best senior thesis award)
  <li><a href="https://scholar.google.com/citations?user=gxRDkLMAAAAJ&hl=en">Samuel Yu</a> (CRA finalist)
  <li><a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a>, now PhD student at Princeton
  <li><a href="https://rulinshao.github.io/">Rulin Shao</a>, now PhD student at University of Washington
  <li><a href="https://xiangfan.io/">Xiang Fan</a>, now PhD student at University of Washington (CRA honorable mention)
  <li><a href="https://jivatneet.github.io/">Jivat Neet</a>, then research fellow at Microsoft Research, now PhD student at UC Berkeley
  <li><a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a>, now PhD student at University of Michigan (CRA honorable mention)
  <li><a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a>, now PhD student at MIT
  <li><a href="https://peter.onrender.com/">Peter Wu</a>, now PhD student at UC Berkeley
  <li><a href="https://dongwonl.com/">Dong Won Lee</a>, now PhD student at MIT
  <li><a href="https://xiangrutang.github.io/">Xiangru Tang</a>, now PhD student at Yale
  <li><a href="https://terranceliu.github.io/">Terrance Liu</a>, now PhD student at CMU
  <li><a href="https://shpark.org/">Seong Hyeon Park</a>, now PhD student at KAIST
  <li><a href="https://www.linkedin.com/in/cmao/">Chengfeng Mao</a>, now PhD student at MIT
  <li><a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, then PhD student at University of Tokyo, now PostDoc at MIT
  <li>Irene Li, now at SoundHound (CRA honorable mention)
</ul>

<h2 id="Talks">Academic Talks</h2>

<ul style="list-style-type:disc">

  <li><b>The Future of Large Language Models: Multimodality and Safety</b></li>
  American Society for Clinical Pharmacology & Therapeutics Annual Meeting, March 2024<br/>
  ACM Multimedia Workshop on Multimodal and Responsible Affective Computing, Oct 2023<br/>
  Microsoft Research, July 2023<br/>
  IBM Zurich, March 2023

  <li><b>Foundations of Multimodal Machine Learning: Principles, Challenges, and Open Questions</b></li>
  African Masters of Machine Intelligence, July 2023<br/>
  CIFAR DLRL Summer School, July 2023<br/>
  ICLR Workshop on Multimodal Representation Learning, April 2023<br/>
  Harvard Medical School AI for Pathology Lab, Oct 2022<br/>
  Heidelberg Laureate Forum, Sept 2022<br/>
  UC Berkeley Speech Group, Sept 2022<br/>
  Stanford University MedAI Group, Sept 2022<br/>
  National University of Singapore, Aug 2022<br/>
  Amazon AI, Aug 2022<br/>
  Allen Institute of AI & University of Washington, June 2022<br/>
  Carnegie Mellon University, May 2022<br/>
  <a href="https://drive.google.com/file/d/1nj5_WSeRPv0eI--03Sh7U9XY_JgIcmmO/view?usp=sharing">[slides1]</a> <a href="https://drive.google.com/file/d/1a7POA_l1eyLpX8J_Wf-BxHk02xue1hZV/view?usp=sharing">[slides2]</a>

  <li><b>Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness</b></li>
  Peking University, March 2023<br/>
  Models of Consciousness Conference, Sept 2022<br/>
  International Joint Conference on Theoretical Computer Science, Aug 2022<br/>
  <a href="https://docs.google.com/presentation/d/1Fm0ugrxYzqxXQTqZ0Y3Qg85OZt-EdpYz/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">[slides]</a>

  <li><b>Towards Real-World Social AI</b></li>
  Facebook Fellowship Summit, Sept 2021<br/>
  DeepMind Multimodal Team, Sept 2021<br/>
  IJCAI Workshop on Multimodal Analytics, Aug 2021<br/>
  Big Data and AI Conference, July 2021<br/>
  Agency for Science, Technology and Research Singapore, June 2021<br/>
  Adobe Research, Jan 2021<br/>
  Carnegie Mellon University, Oct 2020<br/>
  <a href="slides/talk2021_social_AI.pdf">[slides]</a>

  <li><b>Think Locally, Act Globally: Federated Learning with Local and Global Representations</b></li>
  Agency for Science, Technology and Research Singapore, June 2021<br/>
  NeurIPS 2019 Workshop on Federated Learning, Dec 2019<br/>
  <a href="https://docs.google.com/presentation/d/1FILIzRxkSTmWa-8dit6EN64SRKhNB1eYu9FAtyog59I/edit?usp=sharing">[slides]</a>

  <li><b>Computational Modeling of Human Multimodal Language</b></li>
  Google Research, July 2019<br/>
  RIKEN Artificial Intelligence Project Tokyo Machine Learning Seminar, Jan 2019<br/>
  RIKEN Artificial Intelligence Project Kyoto Machine Learning Seminar, Dec 2018<br/>
  ACL 2018, July 2018<br/>
  CMU Machine Learning Department Data Analysis Project Presentation, Apr 2018<br/>
  <a href="slides/2018_hml_slides.pdf">[slides]</a>

</ul>

<br/>
I have an Erd&#337;s number of 3 (Paul Erd&#337;s &rarr; Giuseppe Melfi &rarr; Erik Cambria &rarr; Paul Pu Liang).<br/>
This page has been accessed at least <a href="https://stuff.mit.edu/doc/counter-howto.html"><img src="https://stuff.mit.edu/cgi/counter/pliang" alt="several" style="PADDING-TOP: 20px"></a> times since Feb 8, 2018.

<left>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=100&t=n&d=DOV03OEja9kGRRSbtFesj7DY1F0BQdVjs4Wtk25YT4Q"></script>
</left>
</body>
</font>
</html>
