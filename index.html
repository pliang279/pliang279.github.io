<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Paul Liang, MIT</title>
  <meta name="Paul Liang's Homepage" http-equiv="Content-Type" content="Paul Liang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Paul Liang</pageheading><br>
    <b>email</b>:&nbsp ppliang (at) mit (dot) edu<br>
    <b>office</b>:&nbsp Wiesner Building E15-392
  </p>

  <tr>
    <td width="33%" valign="top"><img src="images/paul-liang-headshot-small.jpg" width="70%" class="center" style="border-radius:15px">
    <p align=center>
    <a href="cv_03_2025.pdf" target="_blank">CV</a> |
    <a href="bio.txt" target="_blank">Bio</a> |
    <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a> <br/>
    <a href="https://github.com/pliang279">Github</a> |
    <a href="https://twitter.com/pliang279">Twitter</a> <br/>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/pliang279?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @pliang279</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

    <a href="papers/research_statement_paul_liang_2024.pdf">Research</a>, <a href="papers/teaching_statement_paul_liang_2024.pdf">teaching</a>, <a href="papers/diversity_statement_paul_liang_2024.pdf">diversity</a> statements<br/>
    </p>

    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am an Assistant Professor at the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>, where I direct the <a href="https://www.media.mit.edu/groups/multisensory-intelligence/overview/">Multisensory Intelligence</a> research group.
    </p>
    
    <p>
    In summer 2024, I was a visiting researcher in the <a href="https://simons.berkeley.edu/programs/summer-cluster-ai-psychology-neuroscience">AI, psychology, and neuroscience program</a> at UC Berkeley's Simons Institute for the Theory of Computing.
    Previously, I received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
    </p>

    <p><b> Prospective students:</b> I am hiring at all levels (post-docs, PhDs, masters, undergrads, and visitors). If you want to join MIT as a graduate student, please apply through the programs in <a href="https://www.media.mit.edu/graduate-program/about-media-arts-sciences/">Media Arts & Sciences</a>, <a href="https://www.eecs.mit.edu/academics/graduate-programs/">EECS</a>,
    or <a href="https://idss.mit.edu/academics/ses_doc/">IDSS</a>, and mention my name in your application.<br/>
    I'm also happy to collaborate and answer questions about my research and MIT programs, I especially encourage students from underrepresented groups to reach out.<br/>
    <br/>
      Some recent talks:<br/><br/>
      <iframe width="256" height="144" src="https://www.youtube.com/embed/n1OaYHnzCuE" title="EI Seminar - Paul Liang - Foundations of High-Modality Multisensory AI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      <iframe width="256" height="144" src="https://www.youtube.com/embed/fVThLBeEWn0" title="Foundations of Multisensory Artificial Intelligence by Prof. Paul Liang from MIT" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      
    </p>
    </td>
  </tr>
</table>

<hr/>


<hr/>

<div class='section_div' id='ResearchGroup'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td colspan="2"><sectionheading><a href="https://www.media.mit.edu/groups/multisensory-intelligence/overview/">Multisensory Intelligence</a> Group</sectionheading></td></tr>
<tr><td colspan="2">    
    <div class='embed-content'>
        <p align="center"><iframe height="360" width="640" src="https://player.vimeo.com/video/1036876426?h=76486a295f&badge=0&autopause=0&player_id=0&app_id=58479"></iframe></p>
    </div>
  <p>
  Our group studies the foundations of multisensory AI and its impact on the human experience, through three complementary thrusts:
  </p>

  <p>
  (1) Foundations of multisensory AI: The science and engineering of AI systems that can learn and interact with the world through integrating diverse sensory channels.
  </p>

  <p>
  (2) Enhancing human experiences: Designing interactive AI technologies to augment human capabilities and improve overall well-being.
  </p>

  <p>
  (3) Real-world human-AI interaction: Quantifying and mitigating real-world societal concerns for responsible deployment.
  </p>
</td></tr>

  <tr>
    <td rowspan="1" width="50%">
      <b>Group</b><br>
      <a href="https://chanakyaekbote.netlify.app/">Chanakya Ekbote</a> (MAS)<br>
      <a href="https://dd.works/">David Dai</a> (MAS)<br>
      <a href="https://megantj.github.io/">Megan Tjandrasuwita</a> (EECS PhD, co-advised with Armando Solar-Lezama)<br>
      <a href="https://scholar.google.com/citations?user=HiPCZxEAAAAJ&hl=en">Ao Qu</a> (IDSS PhD, co-advised with Jinhua Zhao)<br>
      <a href="https://www.linkedin.com/in/devin-murphy-78947318b/">Devin Murphy</a> (EECS MEng, co-advised with Wojciech Matusik)<br>
      <a href="https://scholar.google.com/citations?user=1zL7Q64AAAAJ&hl=en">Lily Chen</a>, <a href="https://www.linkedin.com/in/dewei-feng">Dewei Feng</a>, <a href="https://www.linkedin.com/in/jimin24/">Jimin Lee</a>, <a href="https://www.linkedin.com/in/peilin-chen-1bb81a22a/">Peilin Chen</a>, <a href="https://web.media.mit.edu/~adithyab/">Adithya Balachandran</a> (EECS MEngs)<br>
      <a href="https://sites.google.com/mit.edu/msjung">Minseok Jung</a> (IDSS MS, co-advised with Lalana Kagal)<br>
      <a href="https://stevenshinechen.github.io/">Steven Chen</a>, <a href="https://www.linkedin.com/in/hengzhi-li-89a17418b/">Hengzhi Li</a> (UROPs)<br>
      <a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, <a href="https://aaronhan223.github.io/">Aaron Han</a> (Visiting researchers)<br>
    </td>
  </tr>
  
  <tr><td colspan="2">
      <b>Former students</b><br>
      <a href="https://haofeiyu.me/">Haofei Yu</a>, now PhD student at UIUC<br>
      <a href="https://rpandey.tech/">Rohan Pandey</a>, now researcher at Open AI (best CMU senior thesis award)<br>
      <a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a>, now PhD student at Princeton<br>
      <a href="https://rulinshao.github.io/">Rulin Shao</a>, now PhD student at University of Washington<br>
      <a href="https://xiangfan.io/">Xiang Fan</a>, now PhD student at the University of Washington (CRA outstanding undergrad researcher honorable mention)<br>
      <a href="https://jivatneet.github.io/">Jivat Neet</a>, then research fellow at Microsoft Research, now PhD student at UC Berkeley<br>
      <a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a>, now PhD student at the University of Michigan (CRA outstanding undergrad researcher honorable mention)<br>
      <a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a>, now PhD student at MIT<br>
      <a href="https://peter.onrender.com/">Peter Wu</a>, now PhD student at UC Berkeley<br>
      <a href="https://dongwonl.com/">Dong Won Lee</a>, now PhD student at MIT<br>
      <a href="https://terranceliu.github.io/">Terrance Liu</a>, now PhD student at CMU<br>
      <a href="https://www.linkedin.com/in/cmao/">Chengfeng Mao</a>, now PhD student at MIT<br>
      <a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, then PhD student at the University of Tokyo, now PostDoc at MIT<br>
    </td></tr>
  <tr><td></td></tr>
</table>
</div>

<hr/>

<div class='section_div' id='Teaching'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td><sectionheading>Teaching</sectionheading></td></tr>
<tr><td>
  Spring 2025: <a href="https://mit-mi.github.io/how2ai-course/spring2025/">MIT MAS.S60 How to AI (Almost) Anything</a> <br/>
  Spring 2025: <a href="https://introml.mit.edu/spring25/">MIT 6.390 Introduction to Machine Learning</a> <br/>
  Spring 2024: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Daniel Fried <br/>
  Fall 2023: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Summer 2023: African Masters Of Machine Intelligence course on Multimodal AI (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>)</br>
  2022-2023: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on Multimodal ML</a> at ICML, ICMI, CVPR, NAACL with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">CMU 11-866 Artificial Social Intelligence</a>, with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Fall 2022: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Spring 2022: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency, Amir Zadeh <br/></td></tr>
<tr><td></td></tr>
</table>
</div>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Selected publications </sectionheading>(see full list on <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2503.07667">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/climb.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2503.07667" id="climb">
    <heading>CLIMB: Data Foundations for Large-Scale Multimodal Clinical Foundation Models</heading></a><br>
    David Dai, Peilin Chen, Malinda Lu, Daniel Li, Haowen Wei, Hejie Cui, Paul Pu Liang<br>
    ICML 2025<br>
    </p>

    <div class="paper" id="climb">
    <a href="https://arxiv.org/abs/2503.07667">arXiv</a> |
    <a href="https://github.com/DDVD233/climb">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2502.16282">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/alignment.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2502.16282" id="alignment">
    <heading>Understanding the Emergence of Multimodal Representation Alignment</heading></a><br>
    Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, Paul Pu Liang<br>
    ICML 2025<br>
    </p>

    <div class="paper" id="alignment">
    <a href="https://arxiv.org/abs/2502.16282">arXiv</a> |
    <a href="https://github.com/MeganTj/multimodal_alignment">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2502.16671">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mime.mp4" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2502.16671" id="mime">
    <heading>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</heading></a><br>
    Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang<br>
    arXiv 2025<br>
    </p>

    <div class="paper" id="mime">
    <a href="https://arxiv.org/abs/2502.16671">arXiv</a> |
    <a href="https://github.com/MIT-MI/MimeQA">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2503.16434">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/sketchpad.mov" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2503.16434" id="sketchpad">
    <heading>Interactive Sketchpad: An Interactive Multimodal System for Collaborative, Visual Problem-Solving</heading></a><br>
    Jimin Lee, Steven Chen, Paul Pu Liang<br>
    CHI 2025 Late-Breaking Work<br>
    </p>

    <div class="paper" id="sketchpad">
    <a href="https://arxiv.org/abs/2503.16434">arXiv</a> |
    <a href="https://stevenshinechen.github.io/interactivesketchpad/">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2410.16719">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/evogen.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2410.16719" id="evogen">
    <heading>Progressive Compositionality In Text-to-Image Generative Models</heading></a><br>
    Evans Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang<br>
    ICLR 2025 <font color="#DC143C">(spotlight) </font><br>
    </p>

    <div class="paper" id="evogen">
    <a href="https://arxiv.org/abs/2410.16719">arXiv</a> |
    <a href="https://github.com/evansh666/EvoGen">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.03418">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/hemm.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2407.03418" id="hemm">
    <heading>HEMM: Holistic Evaluation of Multimodal Foundation Models</heading></a><br>
    Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2024<br>
    </p>

    <div class="paper" id="hemm">
    <a href="https://arxiv.org/abs/2407.03418">arXiv</a> |
    <a href="https://github.com/pliang279/HEMM">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2209.03430">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/survey.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2209.03430" id="survey">
    <heading>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</heading></a><br>
    Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    ACM Computing Surveys 2024, Tutorials at ICML & ICMI 2023, CVPR & NAACL 2022<br>
    </p>

    <div class="paper" id=“survey”>
    <a href="https://arxiv.org/abs/2209.03430">arXiv</a> |
    <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">website</a> |
    <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">videos</a>
    </div>
  </td>
</tr>  
  
  
<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.12247">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/pid.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2302.12247" id="pid">
    <heading>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</heading></a><br>
    Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nick Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2023<br>
    </p>

    <div class="paper" id="pid">
    <a href="https://arxiv.org/abs/2302.12247">arXiv</a> |
    <a href="https://github.com/pliang279/PID">code</a>
    </div>
  </td>
</tr>
  
  
<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2203.01311">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/highmmt.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2203.01311" id="highmmt">
    <heading>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    TMLR 2022<br>
    </p>

    <div class="paper" id="highmmt">
    <a href="https://arxiv.org/abs/2203.01311">arXiv</a> |
    <a href="https://github.com/pliang279/HighMMT">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="multibench">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/multibench.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2107.07502" id="multibench">
    <heading>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2021, JMLR Open Source Software 2022<br>
    </p>

    <div class="paper" id="multibench">
    <a href="https://arxiv.org/abs/2107.07502">arXiv</a> |
    <a href="https://multibench.readthedocs.io/en/latest/">website</a> |
    <a href="https://github.com/pliang279/MultiBench">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.13219">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lmbias.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.13219" id="lmbias">
    <heading>Towards Understanding and Mitigating Social Biases in Language Models</heading></a><br>
    Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICML 2021<br>
    </p>

    <div class="paper" id="lmbias">
    <a href="https://arxiv.org/abs/2106.13219">arXiv</a> |
    <a href="https://github.com/pliang279/LM_bias">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2001.01523">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/fl.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2001.01523" id="fl">
    <heading>Think Locally, Act Globally: Federated Learning with Local and Global Representations</heading></a><br>
    Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award) </font> <br>
    </p>

    <div class="paper" id="fl">
    <a href="https://arxiv.org/abs/2001.01523">arXiv</a> |
    <a href="https://github.com/pliang279/LG-FedAvg">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1906.00295">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mult.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1906.00295" id="mult">
    <heading>Multimodal Transformer for Unaligned Multimodal Language Sequences</heading></a><br>
    Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ACL 2019<br>
    </p>

    <div class="paper" id="mult">
    <a href="https://arxiv.org/abs/1906.00295">arXiv</a> |
    <a href="https://github.com/yaohungt/Multimodal-Transformer">code</a>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://aclanthology.org/P18-1208/">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/006.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://aclanthology.org/P18-1208/" id="mosei">
    <heading>Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency<br>
    ACL 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="mosei">
    <a href="https://aclanthology.org/P18-1208/">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a>
    </div>
  </td>
</tr>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>

</body>

</html>
