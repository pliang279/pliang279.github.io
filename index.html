
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN">
<html>
<head>
<title>Paul Pu Liang</title>
<style type="text/css">
	body
	{
		width:1400px;
		text-align: center;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size:16px;
		background-color: #FFF;
	}
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	table
	{
		padding: 5px;
	}

	table.pub_table,td.pub_td1,td.pub_td2
	{
		border-collapse: collapse;
		border-bottom: 0px solid #9B9B9B;
		padding-bottom: 10px;
		padding-top: 10px;
		padding-left: 10px;
		width: 1100px;
	}
	td.pub_td1
	{
		width:100px;
	}
	td.pub_td2
	{
	}
	td.year_heading
	{
		color: #3B3B3B;
		font-weight: 700;
		font-size:20px;
	}
	tr {
		background-color: #FFF;
	}

	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
		text-align: left;
		position: relative;
		background-color: #FFF;
	}
	div#DocInfo
	{
		color: #9B9B9B;
		height: 128px;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #000;
		margin-bottom: 20px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
		font:11px helvetica,sans-serif;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
		font:11px helvetica,sans-serif;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
	}
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	.section_div {
		background-color: #FFF;
		padding: 10px 10px 10px 10px;
		margin: 10px 10px 10px 10px;
		//border: 1px solid #AAA;
	}
	body {
		background-color: #FFF;
	}
	#personal_info {
		background-color: #FFF;
	}
	p.announcement {
		padding: 10px;
		background-color: #EEE;
	}
	img.teaser_img {
		width: 256px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.teaser_img2 {
		width: 206px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.teaser_img3 {
		width: 186px;
		display: block;
    margin-left: auto;
    margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
	}
	img.photo_of_me {
		border-radius: 20px;
	}
	div.teaser_img_div {
		width: 286px;
	}
	table.personnel td {
		padding: 16px;
		vertical-align: top
	}

	p.research_agenda_list {
		margin: 5px;
	}
  .expandable-text {
    max-height: 26px;
    overflow: hidden;
    transition: max-height 0.5s;
  }
  .expanded {
    max-height: 1000px;
  }

</style>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24665197-4', 'auto');
  ga('send', 'pageview');

</script>

</head>


<body>
	<div id="container">

	<div class='section_div'>
	<table id="personal_info">
	<tr>
	<td><img class="photo_of_me" src="images/photo2023_small.jpeg" width=180px style="border: 1px solid black; float:left; margin-right:15px"/></td>
	<td>
	<div id="DocInfo">
		<h1>Paul Pu Liang</h1>
		ppliang(at)mit.edu<br>
		<a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a> / <a href="https://github.com/pliang279">GitHub</a> / <a href="https://twitter.com/pliang279">Twitter</a>
	</div><br>
	</td>
	</tr>
	</table>

<div class='section_div'>
	<h2>About me</h2>
	<p>I am an Assistant Professor in the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>
        <br><br>
        Previously, I spent a year as a visiting research scientist at OpenAI, and before that I was a postdoctoral scholar with <a href="http://www.eecs.berkeley.edu/~efros/">Alyosha Efros</a> in the EECS department at UC Berkeley. I completed my Ph.D. in Brain & Cognitive Sciences at MIT, under the supervision of <a href="http://persci.mit.edu/people/adelson">Ted Adelson</a>, where I also frequently worked with <a href="http://cvcl.mit.edu/Aude.htm">Aude Oliva</a>. I received my undergraduate degree in Computer Science from Yale, where I got my start on research working with <a href="http://www.yale.edu/perception/Brian/">Brian Scholl</a>. A longer bio is <a href="http://web.mit.edu/phillipi/www/bio.html">here</a>.
		<br>
		<!--<br>I study why we represent the world the way we do, and how we can replicate these abilities in machines.--></p>
	Quick links: <a href="http://web.mit.edu/phillipi/www/all_papers.html">Papers</a> / <a href="#Courses">Courses</a> / <a href="http://web.mit.edu/phillipi/www/resources.html">Talks</a> / <a href="http://web.mit.edu/phillipi/www/writing.html">Writing</a> / <a href="#ResearchGroup">Research Group</a><br>
	<br>
</div>
<hr>

<!--<p class='announcement'>
	<b>Prospective students:</b> If you are interested in joining my group, there is no need to send an email (and I may not have capacity to reply). Please instead just
	apply through the <a href="https://gradapply.mit.edu/eecs/apply/login/?next=/eecs/">EECS admissions website</a> and indicate
	your interest in my group in your application. This will get to me and makes things easier on my end.
</p>
</div>
<hr>-->

<div class='section_div'>
	&#11088; <h2 style="display: inline;">News</h2> &#11088;<br><br>

	&#x2192; <a href="http://web.mit.edu/phillipi/www/cvpr2024.html">Our papers, talks, and workshops at CVPR 2024</a><br><br>

<hr>
	<table>
		<tr>
			<td>
				<a href="https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/"><img width=240px src="http://web.mit.edu/phillipi/www/images/fcv_book_cover.png"/></a>
			</td>
			<td style="vertical-align: middle;">
				<div style="margin: 10px 10px;">
					Our computer vision textbook is finished!<br><br>
					Lots of things have happened since we started thinking about this book in November 2010; yes, it has taken us more than 10 years to write this book. Our initial goal was to write a large book that provided a good coverage of the field. Unfortunately, the field of computer vision is just too large for that. So, we decided to write a small book instead, limiting each chapter to no more than five pages. Writing a short book was perfect because we did not have time to write a long book and you did not have time to read it. Unfortunately, we have failed at that goal, too. This book covers foundational topics within computer vision, with an image processing and machine learning perspective. The audience is undergraduate and graduate students who are entering the field, but we hope experienced practitioners will find the book valuable as well.<br>
					<br>
					<a href="https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/">Foundations of Computer Vision</a><br>
					Antonio Torralba, Phillip Isola, William F. Freeman<br>
				MIT Press
			</div>
			</td>
		</tr>
	</table>
	<!--&#x2192; Link to <a href="http://web.mit.edu/phillipi/www/neurips2023.html">our papers, talks, and workshops at NeurIPS 2023</a><br><br>
	&#x2192; <a href="https://github.com/f3rm/f3rm">Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</a> won the best paper award at CoRL 2023. Congrats to the authors!<br>-->
	<!--<b>Our work at ICML 2023</b><br>
	&#x2192; Paper: <a href="https://www.tongzhouwang.info/quasimetric_rl/">Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning<a><br>
	&#x2192; Paper: <a href="https://minyoungg.github.io/vqtorch/">Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks<a><br>
	&#x2192; Workshop: <a href="https://deployinggenerativeai.github.io/">Challenges in Deployable Generative AI<a><br>-->
	<!--<b>Our work at CVPR 2023: <a href="http://web.mit.edu/phillipi/www/cvpr2023.html">link</a></b><br>-->
	<!--&#x2192; I rewrote the description of our lab's research agenda. Like everyone else, we are reorienting, given the pace of recent progress.
	We will focus more on the <i>science</i> of intelligence,
	and on the positive integration of AI systems into society. &#128071;-->

	<!--a fascinating and mysterious new kind of intelligence has suddenly arrived on earth -- GPT and its ilk -- and it is critical that we develop a good understanding of why these things
	work, and where they might go wrong. &#128071;--><!--We will also be working on tools to help integrate AI in a positive way into society. &#128071;-->
	<!--
	It's PhD applications season! Please see info about my group's openings <a href="http://web.mit.edu/phillipi/www/applicant_info.html">here</a>.<br>
	<br>
	<b>Come see our stuff at NeurIPS 2022!</b><br>
	&#x2192; Competition: <a href="https://www.aicrowd.com/challenges/neurips-2022-the-neural-mmo-challenge">NeurIPS 2022 Neural MMO Challenge</a><br>
	&#x2192; Paper: <a href="https://swamiviv.github.io/semantic_uncertainty_intervals/">Semantic uncertainty intervals for disentangled latent spaces<a><br>
	&#x2192; Paper: <a href="https://mbaradad.github.io/shaders21k/">Procedural Image Programs for Representation Learning</a><br>
	&#x2192; Paper: <a href="https://openreview.net/pdf?id=yipUuqxveCy">Offline Multi-Agent Reinforcement Learning with Knowledge Distillation</a><br>
	&#x2192; Workshop Paper (<a href="https://www.neurreps.org/">NeurReps workshop</a>): <a href="https://www.tongzhouwang.info/interval_quasimetric_embedding/">Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings</a><br>
	&#x2192; Workshop Paper (<a href="https://tsrml2022.github.io/">TSRML workshop</a>): <a href="https://openreview.net/pdf?id=GwvWm56hnN">Real world relevance of generative counterfactual explanations</a><br>
	&#x2192; Workshop Talk (<a href="https://sslneurips22.github.io/">SSL workshop</a>): When faking your data actually helps - Self-supervised learning from GANs, NeRFs, and Noise<br>
	&#x2192; Workshop Talk (<a href="https://www.svrhm.com/">SVRHM workshop</a>): Generating Imagery Optimized for Human Consumption
	-->
	<!--&#x2192; Competition We are running a NeurIPS competition on large-scale multiagent learning, on the <a href="https://neuralmmo.github.io/build/html/rst/landing.html">Neural MMO platform</a>. Check it out <a href="https://www.aicrowd.com/challenges/neurips-2022-the-neural-mmo-challenge">here</a>.-->
	<!--&#x2192; We are running a CVPR competition on monitoring deforestation in the Amazon rainforest, using multimodal sensory data. Info and submission website <a href="https://sites.google.com/view/rainforest-challenge">here</a>.<br>-->
	<!--&#x2192; We are running an IJCAI competition on large-scale multiagent learning, on the <a href="https://jsuarez5341.github.io/neural-mmo/build/html/rst/userguide.html">Neural MMO platform</a>. Check it out <a href="https://www.aicrowd.com/challenges/ijcai-2022-the-neural-mmo-challenge">here</a>.-->
	<!--<p>&#9659; New blog post on our work on <a href="https://blog.openai.com/evolved-policy-gradients/">Evolved Policy Gradients</a>.<br>
	&#9659; I am co-organizing a <a href="https://sites.google.com/view/cvpr2018tutorialongans/">tutorial on GANs at CVPR 2018</a>.</p>
	<p>I recently co-organized the <a href="http://vui.eecs.berkeley.edu/">2nd Workshop on Visual Understanding for Interaction</a> at CVPR 2017. Talk slides coming soon!</p>-->
</div>
<hr>


<div class='section_div' id='ResearchGroup'>
<h2>Research Group</h2>

The goal of our group is to <b>scientifically understand intelligence</b>. We are especially interested in human-like intelligence,
i.e. intelligence that is built out of <i>deep nets</i>, is highly <i>adaptive</i> and <i>general-purpose</i>, and is <i>emergent</i> from <i>embodied</i> interactions in rich ecosystems.<br>
<br>
Questions we are studying include the following, which you can click on to expand:<br>
<br>
<!---* <b>Deep nets</b>: Why do neural networks work so well? And when might they still fail?<br>
* <b>Emergence</b>: How can intelligence emerge in the first place, without imitating an existing intelligence?<br>
* <b>Embodiment</b>: To what extent is physical embodiment, and interaction, useful or necessary for intelligence?<br>
* <b>Generality</b>: What training regimes and data distributions result in more general-purpose intelligence?<br>
* <b>Multiagent</b>: What is the role of culture and social interaction in intelligence?<br>--->
<div class="expandable-text" onclick="this.classList.toggle('expanded')">
	<p class="research_agenda_list">&#9656; <i>Science of deep learning</i>: Why do large models find solutions that generalize? What structures improve generalization? And when might these methods still fail?</p>
	<p>Representative projects: <a href="https://minyoungg.github.io/overparam/">Low-rank bias</a>, <a href="https://www.tongzhouwang.info/quasimetric/">Quasimetric learning</a>, <a href="https://hobbitlong.github.io/InfoMin/">What makes for good views for contrastive learning</a></p>
</div>
<div class="expandable-text" onclick="this.classList.toggle('expanded')">
	<p class="research_agenda_list">&#9656; <i>Emergent intelligence</i>: How can intelligence emerge from data and tasks, and how can it emerge without imitating another intelligence's cultural artifacts?</p>
	<p>Representative projects: <a href="https://neuralmmo.github.io/">Neural MMO</a>, <a href="https://kvfrans.com/static/powder/">PowderWorld</a></p>
</div>
<div class="expandable-text" onclick="this.classList.toggle('expanded')">
	<p class="research_agenda_list">&#9656; <i>Embodied intelligence</i>: To what extent is physical embodiment, in interactive environments, useful or necessary for intelligence?</p>
	<p>Representative projects: <a href="https://yilundu.github.io/crl/">Embodied representation learning</a>, <a href="https://yenchenlin.me/mira/">Mental imagery for robots</a></p>
</div>
<div class="expandable-text" onclick="this.classList.toggle('expanded')">
	<p class="research_agenda_list">&#9656; <i>Synthetic data and environments</i>: What kinds of training data / environments are most instructive toward achieving robust, aligned, and general intelligence?</p>
	<p>Representative projects: <a href="https://ali-design.github.io/GenRep/">Generative data</a>, <a href="https://mbaradad.github.io/shaders21k/">Procedural data</a>, <a href="https://yenchenlin.me/nerf-supervision/">NeRF data</a></p>
</div>
<div class="expandable-text" onclick="this.classList.toggle('expanded')">
	<p class="research_agenda_list">&#9656; <i>Controllable AI</i>: How can we make AI systems that can be steered, edited, and controlled by human users?</p>
	<p>Representative projects: <a href="https://hjbahng.github.io/visual_prompting/">Visual prompting</a>, <a href="https://ali-design.github.io/gan_steerability/">GAN steering</a>, <a href="https://jingweim.github.io/totems/">Totems</a></p>
</div>
<br>
Our goal in studying these questions is to help equip the world with the tools necessary to bring about a positive integration of AI into society; to understand intelligence so we can prevent its harms and to reap its benefits. <!---This includes preventing the harms of AI and enabling the benefits.
On the former we work on information integrity and AI alignment. On the latter we work on tools for visual art.<br>--->
<!--
Our group studies how to make <i>artificial</i> intelligence more like <i>natural</i> intelligence.
We are especially interested in intelligence that is <b>embodied</b>, <b>emergent</b>, and <b>general-purpose</b>,
all of which are properties that we see in humans and animals.
<br><br>
Topics we currently focus on include
<b>representation learning</b>, <b>generative modeling</b>, and <b>multiagent systems</b>. We also enjoy eclectic applications on
top of these systems, often in vision and graphics, and also study the misuse of these systems, especially toward spreading misinformation.
-->
<br><br>
The lab is part of the broader <a href="https://ei.csail.mit.edu/">Embodied Intelligence</a> and Visual Computing research communities at MIT.
<br><br>
<table class='personnel'>
	<tr>
		<td rowspan="2" width=400px>
			<b>PhD Students</b><br>
			<a href="https://people.csail.mit.edu/cmchan/">Caroline Chan</a><br>
			<a href="https://ssnl.github.io/">Tongzhou Wang</a><br>
			<a href="http://minyounghuh.com/">Minyoung (Jacob) Huh</a><br>
			<a href="https://hjbahng.github.io/">Hyojin Bahng</a><br>
			<a href="https://akarshkumar.com/">Akarsh Kumar</a><br>
			<a href="https://ssundaram21.github.io/">Shobhita Sundaram</a><br>
			<a href="https://www.cs.columbia.edu/~ipc2107/">Ishaan Preetam-Chandratreya</a>
		</td>
		<td>
			<!--
			<b>MEng Students</b><br>
			<br>
			-->
			<b>Postdocs</b><br>
			<a href="https://jeremybernste.in/">Jeremy Bernstein</a><br>
			<a href="https://www.episodeyang.com/">Ge Yang</a><br>
			<a href="https://prafullsharma.net/">Prafull Sharma</a><br>
			<br>
			<b>Undergraduates</b><br>
			Alan Yu<br>
			<a href="https://www.uzpg.me/">Uzay Girit</a><br>
			<br>
		</td>
	</tr>
	<tr>
	</tr>
	<tr>
		<!--<td>
			<b>Affiliates and Collaborators</b><br>
			<a href="http://people.csail.mit.edu/jahanian/">Ali Jahanian</a>, <a href="https://people.eecs.berkeley.edu/~shelhamer/">Evan Shelhamer</a>,
			<a href="https://www.alexandonian.com/">Alex Andonian</a>, Kexin Yi, <a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig</a>,
			<a href="http://www.mit.edu/~lishuang/">Shuang Li</a>, <a href="https://people.csail.mit.edu/davidbau/home/">David Bau</a>,
			<a href="https://ps.is.mpg.de/person/jwulff">Jonas Wulff</a>, <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
			<a href="http://www.sabrina-osmany.com/about">Sabrina Osmany</a>
		</td>-->
		<td colspan=2>
			<b>Former Members and Visitors</b><br>
				Laker Newhouse (UROP), Hannah Gao (UROP), Sage Simhon (MEng), Jeff Li (UROP, MEng), <a href="https://people.csail.mit.edu/jsuarez/">Joseph Suarez</a> (PhD), <a href="http://yenchenlin.me/">Yen-Chen Lin</a> (PhD), <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a> (PhD),
				<a href="https://swamiviv.github.io/">Swami Sankaranarayanan</a> (Postdoc), <a href="https://www.mit.edu/~fus/">Stephanie Fu</a> (UROP, MEng), <a href="http://kvfrans.com/">Kevin Frans</a> (UROP, MEng), <a href="http://people.csail.mit.edu/yonglong/">Yonglong Tian</a> (PhD), <a href="https://jerryngo.com/">Jerry Ngo</a> (Visiting student), <a href="https://taqiyaehsan.github.io/">Taqiya Ehsan<a> (Visiting student),
				<a href="http://people.csail.mit.edu/jahanian/">Ali Jahanian</a> (Research Scientist), Dillon Dupont (UROP), <a href="https://k8xu.github.io/">Kate Xu</a> (UROP), Maxwell Jiang (UROP), <a href="https://toruowo.github.io/">Toru Lin</a> (MEng), Kenny Derek (MEng), <a href="https://yilundu.github.io/">Yilun Du</a> (UROP), Zhongxia Yan (Rotation)
		</td>
	</tr>
	<tr>
		<td colspan=2>Interested in joining the group? Please see info about applying <a href="http://web.mit.edu/phillipi/www/applicant_info.html">here</a>.</td>
	</tr>
</table>

</div>
<hr>


<div class='section_div' id='Courses'>
<h2>Recent Courses</h2>
<a href="https://phillipi.github.io/6.s953">6.s953: Embodied Intelligence</a> (Spring 2024)<br>
<a href="https://phillipi.github.io/6.s898/">6.s898: Deep Learning</a> (Fall 2023)<br>
<a href="https://phillipi.github.io/6.s898/2022">6.s898: Deep Learning</a> (Fall 2022)<br>
<a href="http://6.869.csail.mit.edu/sp22">6.819/6.869: Advances in Computer Vision</a> (Spring 2022)<br>
<a href="https://phillipi.github.io/6.s898/2021/index.html">6.s898: Deep Learning</a> (Fall 2021)<br>
<a href="https://introml.odl.mit.edu/cat-soop/6.036">6.036: Introduction to Machine Learning</a> (Fall 2020)<br>
<!--<a href="https://phillipi.github.io/6.882/2020/">6.882: Embodied Intelligence</a> (Spring 2020)-->
<!--<a href="http://6.869.csail.mit.edu/fa19">6.819/6.869: Advances in Computer Vision</a> (Fall 2019)<br>
<a href="https://phillipi.github.io/6.882/2019/">6.882: Embodied Intelligence</a> (Spring 2019)<br>
<a href="http://6.869.csail.mit.edu/fa18">6.819/6.869: Advances in Computer Vision</a> (Fall 2018)-->
</div>
<br>
<hr>


	<div class='section_div' id='Papers'>

	<h2>New papers (<a href="http://web.mit.edu/phillipi/www/all_papers.html">All papers</a>)</h2>

	<table class="pub_table">

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2405.07987"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/prh_teaser2.png'/></a></div></td>
	 <td class="pub_td2"><b>The Platonic Representation Hypothesis</b><br>Minyoung Huh*, Brian Cheung*, Tongzhou Wang*, Phillip Isola*<br><i>ICML 2024 (Position Paper, Oral)</i>.<br>[<a href="https://arxiv.org/abs/2405.07987">Paper</a>][<a href="https://phillipi.github.io/prh/">Website</a>][<a href="https://github.com/minyoungg/platonic-rep">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2310.07889"><img class="teaser_img3" src='http://web.mit.edu/phillipi/www/images/langnav_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>LangNav: Language as a Perceptual Representation for Navigation</b><br>Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim<br><i>NAACL 2024 (Findings)</i>.<br>[<a href="https://arxiv.org/abs/2310.07889">Paper</a>][<a href="https://github.com/pbw-Berwin/LangNav">Code</a>][<a href="https://huggingface.co/bpan/LangNav-Sim2k-Llama2">Model</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2312.17742"><img class="teaser_img" src='http://web.mit.edu/phillipi/www/images/synclr_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Learning Vision from Models Rivals Learning Vision from Data</b><br>Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, Phillip Isola<br><i>CVPR 2024</i>.<br>[<a href="https://arxiv.org/abs/2312.17742">Paper</a>][<a href="https://github.com/google-research/syn-rep-learn">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2312.04567"><img class="teaser_img3" src='http://web.mit.edu/phillipi/www/images/synthetic_scaling_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Scaling Laws of Synthetic Images for Model Training ... for Now</b><br>Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, Yonglong Tian<br><i>CVPR 2024</i>.<br>[<a href="https://arxiv.org/abs/2312.04567">Paper</a>][<a href="https://github.com/google-research/syn-rep-learn">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://vision-checkup.csail.mit.edu/"><img class="teaser_img3" src='http://web.mit.edu/phillipi/www/images/vision_checkup_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>A Vision Check-up for Language Models</b><br>Pratyusha Sharma*, Tamar Rott Shaham*, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba<br><i>CVPR 2024 (highlight)</i>.<br>[<a href="https://arxiv.org/abs/2401.01862">Paper</a>][<a href="https://vision-checkup.csail.mit.edu/">Website</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://neuralmmo.github.io/"><img class="teaser_img3" src='http://web.mit.edu/phillipi/www/images/nmmo_v2_teaser.jpg'/></a></div></td>
	 <td class="pub_td2"><b>Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning</b><br>Joseph Su&aacute;rez, Phillip Isola, Kyoung Whan Choe, David Bloomin, Hao Xiang Li, Nikhil Pinnaparaju, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de AlcÃ¢ntara, Herbie Bradley, Louis Castricato, Kirsty You, Yuhao Jiang, Qimai Li, Jiaxin Chen, Xiaolong Zhu<br><i>NeurIPS 2023 Track on Datasets and Benchmarks</i>.<br>[<a href="https://arxiv.org/abs/2311.03736">Paper</a>][<a href="https://neuralmmo.github.io/">Website</a>][<a href="https://github.com/neuralmmo">Code</a>][<a href="https://www.aicrowd.com/search?utf8=%E2%9C%93&q=neural+mmo">Competitions</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://f3rm.github.io/"><video width="100%" height="100%" muted autoplay loop style="border-radius:7.5px;border:0px solid #e1e1e1">
                                <source src="http://web.mit.edu/phillipi/www/images/f3rm_teaser.mp4" type="video/mp4">
                            </video></a></div></td><!--<img class="teaser_img2" src='./images/f3rm_teaser.mp4'/>-->
	<td class="pub_td2"><b>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</b><br>William Shen*, Ge Yang*, Alan Yu, Jansen Wong, Leslie Kaelbling, Phillip Isola<br>
		<i>CoRL 2023 (best paper award)</i>.<br>
		[<a href="https://arxiv.org/abs/2308.07931">Paper</a>][<a href="https://f3rm.github.io/">Website</a>][<a href="https://github.com/f3rm/f3rm">Code</a>][<a href="https://www.youtube.com/watch?v=PA9rWWVWsc4&ab_channel=SparseRewards">Video</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://dreamsim-nights.github.io/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/dreamsim_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Learning New Dimensions of Human Visual Similarity using Synthetic Data</b><br>Stephanie Fu*, Netanel Tamir*, Shobhita Sundaram*, Lucy Chai, Richard Zhang, Tali Dekel, Phillip Isola<br><i>NeurIPS 2023 (spotlight)</i>.<br>[<a href="https://arxiv.org/abs/2306.09344">Paper</a>][<a href="https://dreamsim-nights.github.io/">Website</a>][<a href="https://github.com/ssundaram21/dreamsim">Code/Data</a>][<a href="https://colab.research.google.com/drive/1taEOMzFE9g81D9AwH27Uhy2U82tQGAVI?usp=sharing">Colab</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2306.00984"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/stablerep_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners</b><br>Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan<br><i>NeurIPS 2023</i>.<br>[<a href="https://arxiv.org/abs/2306.00984">Paper</a>][<a href="https://github.com/google-research/syn-rep-learn">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://arxiv.org/abs/2305.20088"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/laclip_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Improving CLIP Training with Language Rewrites</b><br>Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, Yonglong Tian<br><i>NeurIPS 2023</i>.<br>[<a href="https://arxiv.org/abs/2305.20088">Paper</a>][<a href="https://github.com/LijieFan/LaCLIP">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://minyoungg.github.io/vqtorch/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/vq_teaser.png'/></a></div></td>
	 <td class="pub_td2"><b>Straightening Out the Straight-Through Estimator: <br>Overcoming Optimization Challenges in Vector Quantized Networks</b><br>Minyoung Huh, Brian Cheung, Pulkit Agrawal, Phillip Isola<br><i>ICML 2023</i>.<br>[<a href="https://arxiv.org/abs/2305.08842">Paper</a>][<a href="https://minyoungg.github.io/vqtorch/">Website</a>][<a href="https://github.com/minyoungg/vqtorch">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://www.tongzhouwang.info/quasimetric_rl/"><img class="teaser_img3" src='http://web.mit.edu/phillipi/www/images/qrl_teaser.gif'/></a></div></td>
	 <td class="pub_td2"><b>Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning</b><br>Tongzhou Wang, Antonio Torralba, Phillip Isola, Amy Zhang<br><i>ICML 2023</i>.<br>[<a href="https://arxiv.org/abs/2304.01203">Paper</a>][<a href="https://www.tongzhouwang.info/quasimetric_rl/">Website</a>][<a href="https://github.com/quasimetric-learning/quasimetric-rl/">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://chail.github.io/persistent-nature/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/persistent_nature_teaser.gif'/></a></div></td>
	 <td class="pub_td2"><b>Persistent Nature: A Generative Model of Unbounded 3D Worlds</b><br>Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, Noah Snavely<br><i>CVPR 2023</i>.<br>[<a href="https://arxiv.org/abs/2303.13515">Paper</a>][<a href="https://chail.github.io/persistent-nature/">Website</a>][<a href="https://github.com/google-research/google-research/tree/master/persistent-nature">Code</a>]
	</td></tr>

	<tr>
	 <td class="pub_td1"><div class="teaser_img_div"><a href="https://kvfrans.com/static/powder/"><img class="teaser_img2" src='http://web.mit.edu/phillipi/www/images/powderworld_teaser2.gif'/></a></div></td>
	 <td class="pub_td2"><b>Powderworld: A Platform for Understanding Generalization via Rich Task Distributions</b><br>Kevin Frans, Phillip Isola<br><i>ICLR 2023 (notable top 25%)</i>.<br>[<a href="https://arxiv.org/abs/2211.13051">Paper</a>][<a href="https://kvfrans.com/static/powder/">Blog + Demo</a>][<a href="https://github.com/kvfrans/powderworld">Code</a>]
	</td></tr>

	</table>

	</div>

<hr>
<center><a href="http://accessibility.mit.edu/">Accessibility</a></center>

</body>

</html>
