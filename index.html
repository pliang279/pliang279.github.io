<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="https://www.cs.cmu.edu/sites/default/files/favicon_0.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Paul Pu Liang, MIT</title>
  <meta name="Paul Liang's Homepage" http-equiv="Content-Type" content="Paul Liang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="970" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Paul Pu Liang</pageheading><br>
    <b>email</b>:&nbsp ppliang (at) mit (dot) edu
  </p>

  <tr>
    <td width="32%" valign="top"><a href="images/photo2023_small.jpeg"><img src="images/photo2023_small.jpeg" width="100%" style="border-radius:15px"></a>
    <p align=center>
    <a href="cv_07_2024.pdf" target="_blank">CV</a> |
    <a href="bio.txt" target="_blank">Bio</a> |
    <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a> <br/>
    <a href="https://github.com/pliang279">Github</a> |
    <a href="https://twitter.com/pliang279">Twitter</a> <br/>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/pliang279?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @pliang279</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am an Assistant Professor at the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>, where I direct the Multisensory Intelligence research group.
    </p>
    
    <p>
    In summer 2024, I was a visiting researcher in the <a href="https://simons.berkeley.edu/programs/summer-cluster-ai-psychology-neuroscience">AI, psychology, and neuroscience program</a> at UC Berkeley's Simons Institute for the Theory of Computing.
    Previously, I received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
    </p>

    <p><b> Prospective students:</b> I am looking to hire students at all levels (post-docs, PhDs, masters, undergrads, and visitors). If you want to join MIT as a graduate student, please apply through the programs in <a href="https://www.media.mit.edu/graduate-program/about-media-arts-sciences/">Media Arts & Sciences</a> or <a href="https://www.eecs.mit.edu/academics/graduate-programs/">EECS</a>, and mention my name in your application.
    I am also happy to collaborate and answer questions about my research and MIT academic programs. If you are interested, please send me an email. I especially encourage students from underrepresented groups to reach out.
    </p>
    </td>
  </tr>
</table>

<hr/>


<hr/>

<div class='section_div' id='ResearchGroup'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td colspan="2"><sectionheading>Research Group</sectionheading></td></tr>
<tr><td colspan="2">
  <p>
  Our group studies the foundations of multisensory AI and its impact on the human experience, through three complementary thrusts:
  </p>

  <p>
  (1) Foundations of multisensory AI: The science and engineering of AI systems that can learn and interact with the world through integrating diverse sensory channels.
  </p>

  <p>
  (2) Enhancing human experiences: Designing interactive AI technologies to augment human capabilities and improve overall well-being. 
  </p>

  <p>
  (3) Real-world human-AI interaction: Quantifying and mitigating real-world societal concerns for responsible deployment.
  </p>  
</td></tr>
  <tr>
    <td rowspan="1" width="50%">
      <b>Graduate Students</b><br>
      <a href="https://www.antonischristou.com/">Antonis Christou</a><br>
      <a href="https://chanakyaekbote.netlify.app/">Chanakya Ekbote</a><br>
      <a href="https://dd.works/">David Dai</a><br>
      <a href="https://sites.google.com/mit.edu/msjung">Minseok "Mason" Jung</a><br>
    </td>
  </tr>
  
  <tr><td colspan="2">
      <b>Former Students</b><br>
      <a href="https://haofeiyu.me/">Haofei Yu</a>, now PhD student at UIUC<br>
      <a href="https://rpandey.tech/">Rohan Pandey</a>, now at Reworkd AI (YC S23) (best senior thesis award)<br>
      <a href="https://scholar.google.com/citations?user=gxRDkLMAAAAJ&hl=en">Samuel Yu</a> (CRA finalist)<br>
      <a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a>, now PhD student at Princeton<br>
      <a href="https://rulinshao.github.io/">Rulin Shao</a>, now PhD student at University of Washington<br>
      <a href="https://xiangfan.io/">Xiang Fan</a>, now PhD student at the University of Washington (CRA honorable mention)<br>
      <a href="https://jivatneet.github.io/">Jivat Neet</a>, then research fellow at Microsoft Research, now PhD student at UC Berkeley<br>
      <a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a>, now PhD student at the University of Michigan (CRA honorable mention)<br>
      <a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a>, now PhD student at MIT<br>
      <a href="https://peter.onrender.com/">Peter Wu</a>, now PhD student at UC Berkeley<br>
      <a href="https://dongwonl.com/">Dong Won Lee</a>, now PhD student at MIT<br>
      <a href="https://xiangrutang.github.io/">Xiangru Tang</a>, now PhD student at Yale<br>
      <a href="https://terranceliu.github.io/">Terrance Liu</a>, now PhD student at CMU<br>
      <a href="https://shpark.org/">Seong Hyeon Park</a>, now PhD student at KAIST<br>
      <a href="https://www.linkedin.com/in/cmao/">Chengfeng Mao</a>, now PhD student at MIT<br>
      <a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, then PhD student at the University of Tokyo, now PostDoc at MIT<br>
      Irene Li, now at SoundHound (CRA honorable mention)<br>
    </td></tr>
  <tr><td></td></tr>
</table>
</div>

<hr/>

<div class='section_div' id='Teaching'>
<table width="100%" width="100%" align="center" border="0" cellspacing="0" cellpadding="8" style="margin-left:15px">
<tr><td><sectionheading>Teaching</sectionheading></td></tr>
<tr><td>
  Spring 2025: MIT How to AI (Almost) Anything (coming soon!) <br/>
  Spring 2025: MIT Introduction to Machine Learning (coming soon!) <br/>
  Spring 2024: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Daniel Fried <br/>
  Fall 2023: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Summer 2023: Multimodal Artificial Intelligence (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>), African Masters Of Machine Intelligence</br>
  2022-2023: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on Multimodal ML</a> at ICML, ICMI, CVPR, NAACL with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">CMU 11-866 Artificial Social Intelligence</a>, with Louis-Philippe Morency <br/>
  Spring 2023: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Fall 2022: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">CMU 11-777 Multimodal Machine Learning</a>, with Louis-Philippe Morency <br/>
  Spring 2022: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">CMU 11-877 Advanced Topics in Multimodal Machine Learning</a>, with Louis-Philippe Morency, Amir Zadeh <br/></td></tr>
<tr><td></td></tr>
</table>
</div>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications </sectionheading>(representative papers are <span class="highlight">highlighted</span>)<br/>&nbsp;&nbsp;&nbsp;last update: July 2024</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.03418">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/hemm.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2407.03418" id="hemm">
    <heading>HEMM: Holistic Evaluation of Multimodal Foundation Models</heading></a><br>
    Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    arXiv 2024<br>
    </p>

    <div class="paper" id="hemm">
    <a href="https://arxiv.org/abs/2407.03418">arXiv</a> |
    <a href="https://github.com/pliang279/HEMM">code</a> |
    <a href="javascript:toggleblock('hemm_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('hemm')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="hemm_abs">Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024hemm,
  title={HEMM: Holistic Evaluation of Multimodal Foundation Models},
  author={Liang, Paul Pu and Goindani, Akshay and Chafekar, Talha and Mathur, Leena and Yu, Haofei and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2407.03418},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.13219">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lmbias.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.13219" id="lmbias">
    <heading>Towards Understanding and Mitigating Social Biases in Language Models</heading></a><br>
    Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICML 2021<br>
    </p>

    <div class="paper" id="lmbias">
    <a href="https://arxiv.org/abs/2106.13219">arXiv</a> |
    <a href="https://github.com/pliang279/LM_bias">code</a> |
    <a href="javascript:toggleblock('lmbias_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('lmbias')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="lmbias_abs">As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1906.00295">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mult.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1906.00295" id="mult">
    <heading>Multimodal Transformer for Unaligned Multimodal Language Sequences</heading></a><br>
    Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ACL 2019<br>
    </p>

    <div class="paper" id="mult">
    <a href="https://arxiv.org/abs/1906.00295">arXiv</a> |
    <a href="https://github.com/yaohungt/Multimodal-Transformer">code</a> |
    <a href="javascript:toggleblock('mult_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mult')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mult_abs">Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{tsai2019multimodal,
  title={Multimodal Transformer for Unaligned Multimodal Language Sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6558--6569},
  year={2019}
}
</pre>
    </div>
  </td>
</tr>
  
  
<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2001.01523">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/fl.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2001.01523" id="fl">
    <heading>Think Locally, Act Globally: Federated Learning with Local and Global Representations</heading></a><br>
    Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award)<br>
    </p>

    <div class="paper" id="fl">
    <a href="https://arxiv.org/abs/2001.01523">arXiv</a> |
    <a href="https://github.com/pliang279/LG-FedAvg">code</a> |
    <a href="javascript:toggleblock('fl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('fl')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="fl_abs">Federated learning is a method of training models on private data distributed over multiple devices. To keep device data private, the global model is trained by only communicating parameters and updates which poses scalability challenges for large models. To this end, we propose a new federated learning algorithm that jointly learns compact local representations on each device and a global model across all devices. As a result, the global model can be smaller since it only operates on local representations, reducing the number of communicated parameters. Theoretically, we provide a generalization analysis which shows that a combination of local and global models reduces both variance in the data as well as variance across device distributions. Empirically, we demonstrate that local models enable communication-efficient training while retaining performance. We also evaluate on the task of personalized mood prediction from real-world mobile data where privacy is key. Finally, local models handle heterogeneous data from new devices, and learn fair representations that obfuscate protected attributes such as race, age, and gender.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2020think,
  title={Think locally, act globally: Federated learning with local and global representations},
  author={Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Allen, Nicholas B and Auerbach, Randy P and Brent, David and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2001.01523},
  year={2020}
}
</pre>
    </div>
  </td>
</tr>
  

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>

</body>

</html>
