<!DOCTYPE html>
<html>
<font face="Verdana" size = "2.5">
<head>

<style type="text/css">
A:link { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:visited { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:active { COLOR: #0903A5; TEXT-DECORATION: none }
A:hover { COLOR: #C80537; TEXT-DECORATION: none; font-weight: none }
img {
  margin-top: -20px;
  margin-bottom: 0px;
}
h1 {
  margin-top: 15px;
  margin-bottom: 15px;
}
h2 {
  margin-top: 10px;
  margin-bottom: 10px;
}
h2 + ul {
  margin-top: -5px;
  margin-bottom: 0px;
}
h2 + ol {
  margin-top: 0px;
  margin-bottom: 20px;
}
h3 + ol {
  margin-top: -10px;
  margin-bottom: -10px;
}
hr {
    display: block;
    height: 1px;
    border: 0;
    border-top: 1px solid #ccc;
    margin: 1em 0;
    padding: 0;
}
</style>

<title>Paul Liang, MIT</title>

</head>

<body>

<img src="images/photo2023_small.jpeg" width="170" height="170" style="float: left; PADDING-TOP: 20px; PADDING-RIGHT: 10px; PADDING-BOTTOM: 0px"/>

<h1>Paul Pu Liang</h1>
Assistant Professor, <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a><br/>
Email: ppliang(at)mit.edu<br/>
<br/>
<br/>

<a href="papers/cv_07_2024.pdf"><font size="+1.5">[CV]</font></a>

<a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ"> <img src="images/gscholar.jpg" width="24" height="24" style="margin-bottom:-7px;PADDING-LEFT:5px;PADDING-RIGHT:5px">[google scholar]</a>

<a href="https://www.linkedin.com/in/paulpuliang/"> <img src="images/linkedin.png" width="20" height="20" style="margin-bottom:-5px;PADDING-LEFT:5px;PADDING-RIGHT:5px">[linkedin]</a> 

<a href="https://github.com/pliang279"> <img src="images/github.png" width="30" height="30" style="margin-bottom:-10px">[github]</a>

<a href="https://twitter.com/pliang279/"> <img src="images/twitter.png" width="30" height="30" style="margin-bottom:-10px">[twitter]</a>

<a href="https://www.instagram.com/lpwinniethepu/"> <img src="images/instagram.png" width="22" height="22" style="margin-bottom:-6px;PADDING-LEFT:5px;PADDING-TOP: 10px"> [instagram]</a>

<br/>
<br/>
<br/>  
<font color="#DC143C">I will be joining MIT as an Assistant Professor in Fall 2024, joint between the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>.</font><br/>
I will direct the new multisensory intelligence research group, which studies the foundations of multisensory AI and its impact on the human experience. My group will work on:<br/>
(1) Foundations of multisensory AI: The science and engineering of AI systems that can learn and interact with the world through integrating diverse sensory channels such as text, speech, audio, video, physical sensors, and physiological messages.<br/>
(2) Impact on the human experience: AI for human physical, emotional, and social well-being, multimedia generative AI to augment human creativity, climate and environment sensing, enhancing human sensory experiences like music, art, cultures, smell, and taste.<br/>
(3) Real-world human-AI interaction: Ensuring fairness, robustness, trust, privacy, and efficiency for responsible deployment.
<br/>
<br/>
I recently received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
I was also fortunate to collaborate with <a href="https://www.cs.cmu.edu/~mblum/">Manuel Blum</a>, <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/lblum.html">Lenore Blum</a>, <a href="https://faisal.ai/">Faisal Mahmood</a>, <a href="https://jmhessel.com/">Jack Hessel</a>, and <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a> at Berkeley, Harvard Medical School, and UW/AI2.
My research was generously supported by a <a href="https://www.siebelscholars.com/scholar-profile/3723/">Siebel Scholars Award</a>, Waibel Presidential Fellowship, <a href="https://research.fb.com/fellows/liang-paul-pu/">Facebook PhD Fellowship</a>, and <a href="https://www.cs.cmu.edu/cmlh/digital-health-archive/cmlh-digital-health-fellows-2021">Center for Machine Learning and Health Fellowship</a>,
and has been recognized by 4 best paper/honorable mention awards at international conferences and workshops.
I love teaching and was honored to receive the <a href="https://www.cs.cmu.edu/~scsfacts/perlis.html">Alan J. Perlis Graduate Student Teaching Award</a> for co-instructing courses on multimodal machine learning.
Previously, I received an M.S. in Machine Learning and a B.S. with University Honors in Computer Science and Neural Computation from CMU.
<br/>
<br/>
<font color="#DC143C"><b>Research opportunities:</b> I am looking to hire students at all levels (post-docs, PhDs, masters, undergrads, and visitors). I am also happy to collaborate and answer questions about my research and MIT academic programs. If you are interested, please send me an email. I especially encourage students from underrepresented groups to reach out.</font>
<br/>

<h2 id="News">News</h2>

<ul style="list-style-type:disc; line-height:140%">

  <li>2023: Excited to release some recent work formalizing and quantifying multimodal interactions from <a href="https://arxiv.org/abs/2302.12247/">statistical (NeurIPS 2023)</a> and <a href="https://arxiv.org/abs/2306.04125/">human (ICMI 2023)</a> perspectives, with applications in <a href="https://arxiv.org/abs/2207.00056/">visualizing and interpreting multimodal models (ICLR 2023)</a>, <a href="https://arxiv.org/abs/2306.05268/">contrastive learning of unique information (NeurIPS 2023)</a>, and <a href="https://arxiv.org/abs/2306.04539/">guarantees for multimodal semi-supervised learning (ICLR 2024)</a>.

  <li>2023: Co-teaching <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">11-777 Multimodal Machine Learning, Fall 2023</a>, course content will be updated on the website. 

  <li>2023: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on multimodal machine learning</a> at ICML 2023, ICMI 2023, CVPR 2022 and NAACL 2022, teaching at CIFAR DLRL summer school and African Masters of Machine Intelligence (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>): check out our <a href="https://arxiv.org/abs/2209.03430">survey paper</a>, <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">slides, and videos</a>.

  <li>2023: Nothing has excited me more than collaborating with and advising great students. I've learned so much from them and I'm excited to watch them embark on their new research agendas - follow their work for more exciting new ideas! <a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a> -> PhD at Princeton, <a href="https://rulinshao.github.io/">Rulin Shao</a> -> PhD at UW, <a href="https://xiangfan.io/">Xiang Fan</a> -> PhD at UW, <a href="https://jivatneet.github.io/">Jivat Neet</a> -> PhD at Berkeley, <a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a> -> PhD at Michigan, <a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a> -> PhD at MIT, <a href="https://peter.onrender.com/">Peter Wu</a> -> PhD at Berkeley, <a href="https://dongwonl.com/">Dong Won Lee</a> -> PhD at MIT, <a href="https://terranceliu.github.io/">Terrance Liu</a> -> PhD at CMU.

  <li>2023: LP and I are teaching 2 new graduate seminar courses: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">11-866 Artificial Social Intelligence</a> and <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">11-877 Advanced Topics in Multimodal Machine Learning</a>.

  <li>2022: Check out course content for <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">11-777 Multimodal Machine Learning, Fall 2022</a>, where LP and I have completely revamped the course content. Also check out fully recorded <a href="https://www.youtube.com/channel/UCqlHIJTGYhiwQpNuPU5e2gg">lecture videos</a> and <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2020/">course content</a> for 11-777 in Fall 2020.</li></li>

  <li>2022: Are you working on multimodal tasks and can't decide on a model? Check out <a href="https://arxiv.org/abs/2203.01311">HighMMT (TMLR 2022)</a>, our attempt at a single multimodal model that can predict sentiment, emotion, humor, disease, robot pose, and more, as well as <a href="https://arxiv.org/abs/2107.07502">MultiBench (NeurIPS 2021)</a> and <a href="https://arxiv.org/abs/2306.16413">MultiZoo (JMLR 2022)</a>, a large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas.</li>

  <li>2021: Extremely honored to have received a <a href="https://research.facebook.com/fellows/liang-paul-pu/">Facebook PhD Fellowship</a> and a <a href="https://www.cs.cmu.edu/cmlh/digital-health-archive/cmlh-digital-health-fellows-2021">Center for Machine Learning and Health Fellowship</a> to support my research in socially intelligent AI! For students applying for graduate fellowships, I uploaded my <a href="papers/research_statement_paul_liang_2020.pdf">statement from the 2020 application</a>.</li>

  <li>2020: Check out the <a href="https://blog.ml.cmu.edu/">CMU Machine Learning Blog</a> - new research and educational content every few weeks on ML research going on at CMU!</li>
</ul>

<h2 id="Publications">Selected Publications</h2>

(* denotes joint first-authors, see <a href="https://pliang279.github.io/papers/">full list of publications here)</a></br>

<h3>Foundations of multimodal machine learning:</h3>

<ul style="line-height:140%">

  <li><b>HEMM: Holistic Evaluation of Multimodal Foundation Models</b></li>
  Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  <a href="https://arxiv.org/abs/2407.03418">[arXiv]</a> <a href="https://github.com/pliang279/HEMM">[code]</a>

  <li><b>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</b></li>
  Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICLR 2024<br/>
  <a href="https://arxiv.org/abs/2306.04539">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</b></li>
  Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2023<br/>
  <a href="https://arxiv.org/abs/2302.12247">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</b></li>
  Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  ACM Computing Surveys, Tutorials at ICML 2023, ICMI 2023, CVPR 2022, NAACL 2022<br/>
  <a href="https://arxiv.org/abs/2209.03430">[arXiv]</a> <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">[tutorial website]</a> <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">[tutorial videos]</a>
</ul>

<h3>Multimodal AI for real-world sensors:</h3>

<ul style="line-height:140%">

  <li><b>IoT-LM: Large Multisensory Language Models for the Internet of Things</b></li>
  Shentong Mo, Ruslan Salakhutdinov, Louis-Philippe Morency, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2407.09801">[arXiv]</a> <a href="https://github.com/Multi-IoT/MultiIoT">[code]</a>

  <li><b>MultiIoT: Benchmarking Machine Learning for the Internet of Things</b></li>
  Shentong Mo, Louis-Philippe Morency, Ruslan Salakhutdinov, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2311.06217">[arXiv]</a> <a href="https://github.com/Multi-IoT/MultiIoT">[code]</a>
</ul>


<h3>Representation learning over multisensory and temporal data:</h3>

<ul style="line-height:140%">

  <li><b>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  TMLR 2022<br/>
  <a href="https://arxiv.org/abs/2203.01311">[arXiv]</a> <a href="https://github.com/pliang279/HighMMT">[code]</a>

  <li><b>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2021, JMLR Open Source Software 2022<br/>
  <a href="https://arxiv.org/abs/2107.07502">[arXiv]</a> <a href="https://arxiv.org/abs/2306.16413">[software]</a> <a href="https://multibench.readthedocs.io/en/latest/">[website]</a> <a href="https://github.com/pliang279/MultiBench">[code]</a>

  <li><b>Multimodal Transformer for Unaligned Multimodal Language Sequences</b></li>
  Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ACL 2019<br/>
  <a href="https://arxiv.org/abs/1906.00295">[arXiv]</a> <a href="https://github.com/yaohungt/Multimodal-Transformer">[code]</a>
</ul>

<h3>Socially-intelligent AI:</h3>

<ul style="line-height:140%">

  <li><b>Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions</b></li>
  Leena Mathur, Paul Pu Liang, Louis-Philippe Morency<br/>
  <a href="https://arxiv.org/abs/2404.11023">[arXiv]</a> <a href="https://github.com/l-mathur/social-ai">[code]</a>

  <li><b>Computational Modeling of Human Multimodal Language: The MOSEI Dataset and Interpretable Dynamic Fusion</b></li>
  Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  Master's Thesis, CMU Machine Learning Data Analysis Project 2018 <font color="#DC143C">(first runner-up award)</font><br/>
  <a href="papers/dap2018_mosei.pdf">[paper]</a> <a href="slides/dap2018_mosei_slides.pdf">[slides]</a> <a href="posters/dap2018_mosei_poster.pdf">[poster]</a>

  <li><b>Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning</b></li>
  Minghai Chen*, Sen Wang*, Paul Pu Liang*, Tadas Baltru&scaron;aitis, Amir Zadeh, Louis-Philippe Morency<br/>
  ICMI 2017 <font color="#DC143C">(oral, honorable mention award)</font><br/>
  <a href="https://arxiv.org/abs/1802.00924">[arXiv]</a> <a href="https://drive.google.com/drive/u/0/folders/1NxyFuogyzNFoCH0Zi5aIXUGYKGuSY9TY">[code]</a> <a href="slides/icmi2017_gme_slides.pdf">[slides]</a>
</ul>
  
<h3>Multimodal applications in health and wellness:</h3>

<ul style="line-height:140%">

  <li><b>Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction</b></li>
  Guillaume Jaume, Anurag Vaidya, Richard Chen, Drew Williamson, Paul Pu Liang, Faisal Mahmood<br/>
  CVPR 2024<br/>
  <a href="https://arxiv.org/abs/2304.06819">[arXiv]</a> <a href="https://github.com/mahmoodlab/SurvPath">[code]</a>

  <li><b>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</b></li>
  Paul Pu Liang*, Terrance Liu*, Anna Cai, Michal Muszynski, Ryo Ishii, Nick Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2021 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2106.13213">[arXiv]</a>
</ul>

<h3>Real-world human-AI interaction:</h3>

<ul style="line-height:140%">

  <li><b>Towards Understanding and Mitigating Social Biases in Language Models</b></li>
  Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICML 2021<br/>
  <a href="https://arxiv.org/abs/2106.13219">[arXiv]</a> <a href="https://github.com/pliang279/LM_bias">[code]</a>

  <li><b>Towards Debiasing Sentence Representations</b></li>
  Paul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2020<br/>
  <a href="https://arxiv.org/abs/2007.08100">[arXiv]</a> <a href="https://github.com/pliang279/sent_debias">[code]</a>

  <li><b>Think Locally, Act Globally: Federated Learning with Local and Global Representations</b></li>
  Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award)</font><br/>
  <a href="https://arxiv.org/abs/2001.01523">[arXiv]</a> <a href="https://github.com/pliang279/LG-FedAvg">[code]</a>
</ul>

<h2 id="Teaching">Teaching</h2>

<ul style="list-style-type:disc; line-height:140%">

  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2024/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2024, CMU with Daniel Fried <br/>
  <li>Co-Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2023/">11-777 Multimodal Machine Learning</a>, Fall 2023, CMU with Louis-Philippe Morency <br/>
  <li>Instructor: Multimodal Artificial Intelligence (<a href="https://docs.google.com/presentation/d/1WIOY5QCjsJoUO8xZ76pKJyydxJb99gAU/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day1</a>, <a href="https://docs.google.com/presentation/d/1vHo4PcdTJwkRiaj4YbdlIvfdUg0p7JBp/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day2</a>, <a href="https://docs.google.com/presentation/d/183Yaxs-1owDSlg906u01k3kt5gZ6wD6X/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day3</a>, <a href="https://docs.google.com/presentation/d/1vhk81MOJ2qLAtcNK2Lp9xeLL8FCCRc-S/edit?usp=sharing&ouid=112454988971729379076&rtpof=true&sd=true">day4</a>), African Masters Of Machine Intelligence, Summer 2023
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/icml2023/">Tutorials on Multimodal ML</a> at ICML 2023, ICMI 2023, CVPR 2022 and NAACL 2022 with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/asi-course/spring2023/">11-866 Artificial Social Intelligence</a>, Spring 2023, CMU with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2023/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2023, CMU with Louis-Philippe Morency <br/>
  <li>Co-Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2022/">11-777 Multimodal Machine Learning</a>, Fall 2022, CMU with Louis-Philippe Morency <br/>
  <li>Co-Instructor: <a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">11-877 Advanced Topics in Multimodal Machine Learning</a>, Spring 2022, CMU with Louis-Philippe Morency and Amir Zadeh <br/>
  <li>Guest Lecturer: <a href="https://deeplearning-cmu-10707-2022spring.github.io/">10-707 Deep Learning</a>, <a href="https://sites.google.com/andrew.cmu.edu/haii-cmu/">05-618 Human AI Interaction</a>, <a href="https://fall22.mayankgoel.courses/">17-728 Machine Learning and Sensing</a>, Peking University, University of Florida.<br/>
  Lectures on multimodal machine learning <a href="https://drive.google.com/file/d/1IkIRTQfIcp61qqW3dCYKgnpjbN0vRo-U/view">[slides]</a> <a href="https://www.youtube.com/watch?v=9v6Xg5Nk76M&t=3467s&ab_channel=PaulLiang">[video]</a>
  <li>Head TA & Lecturer: <a href="https://cmu-multicomp-lab.github.io/mmml-course/fall2020/">11-777 Multimodal Machine Learning</a>, Fall 2020, CMU. Instructor: Louis-Philippe Morency<br/>
  4 lectures on multimodal tasks <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kemfewi19ex6cc">[slides]</a> <a href="https://www.youtube.com/watch?v=fBYu8I52nVM">[video]</a>, deep generative models <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kglbikewtgy3xt">[slides]</a> <a href="https://www.youtube.com/watch?v=qEbYtPhG768">[video]</a>, reinforcement learning <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kgs9n4o8ft81wp">[slides]</a> <a href="https://www.youtube.com/watch?v=OI02F2XEe_0">[video]</a>, and multimodal RL <a href="https://piazza.com/class_profile/get_resource/kcnr11wq24q6z7/kguil6snycmb">[slides]</a> <a href="https://www.youtube.com/watch?v=UsAgvMC5fRs">[video]</a>.<br/>
  Public videos on YouTube have amassed more than 10000 views.</li>
  <li>Head TA & Lecturer: <a href="https://piazza.com/cmu/fall2019/11777/resources">11-777 Multimodal Machine Learning</a>, Fall 2019, CMU. Instructor: Louis-Philippe Morency<br/>
  2 lectures on reinforcement learning <a href="https://piazza.com/class_profile/get_resource/jv5kp3m5nmu43l/k1tb5xj1jn92ca">[slides]</a> and multimodal RL <a href="https://piazza.com/class_profile/get_resource/jv5kp3m5nmu43l/k1vh2akqdht80">[slides]</a></li>
  <li>TA: <a href="https://sailinglab.github.io/pgm-spring-2019/">10-708 Probabilistic Graphical Models</a>, Spring 2019, CMU. Instructor: Eric Xing</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~10715-f18/">10-715 Advanced Introduction to Machine Learning</a>, Fall 2018, CMU. Instructor: Maria-Florina Balcan</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~roni/10601/">10-601 Introduction to Machine Learning</a>, Fall 2016, CMU. Instructor: Roni Rosenfeld</li>
  <li>TA: <a href="https://www.cs.cmu.edu/~213/">15-213/18-213/15-513 Introduction to Computer Systems</a>, Summer 2016, CMU. Instructor: Brian Railing</li>
</ul>

<h2 id="Group">Research Group</h2>

Some amazing students I've had the pleasure of advising:
<br/>

<ul style="list-style-type:disc; line-height:140%">

  <li><a href="https://haofeiyu.me/">Haofei Yu</a>, now PhD student at UIUC
  <li><a href="https://rpandey.tech/">Rohan Pandey</a>, now at Reworkd AI (YC S23) (best senior thesis award)
  <li><a href="https://scholar.google.com/citations?user=gxRDkLMAAAAJ&hl=en">Samuel Yu</a> (CRA finalist)
  <li><a href="https://scholar.google.com/citations?user=I4p2ikMAAAAJ&hl=en">Yun Cheng</a>, now PhD student at Princeton
  <li><a href="https://rulinshao.github.io/">Rulin Shao</a>, now PhD student at University of Washington
  <li><a href="https://xiangfan.io/">Xiang Fan</a>, now PhD student at University of Washington (CRA honorable mention)
  <li><a href="https://jivatneet.github.io/">Jivat Neet</a>, then research fellow at Microsoft Research, now PhD student at UC Berkeley
  <li><a href="https://scholar.google.com/citations?user=fV5fYpsAAAAJ&hl=en">Yiwei Lyu</a>, now PhD student at University of Michigan (CRA honorable mention)
  <li><a href="https://xiaoyuxin1002.github.io/">Yuxin Xiao</a>, now PhD student at MIT
  <li><a href="https://peter.onrender.com/">Peter Wu</a>, now PhD student at UC Berkeley
  <li><a href="https://dongwonl.com/">Dong Won Lee</a>, now PhD student at MIT
  <li><a href="https://xiangrutang.github.io/">Xiangru Tang</a>, now PhD student at Yale
  <li><a href="https://terranceliu.github.io/">Terrance Liu</a>, now PhD student at CMU
  <li><a href="https://shpark.org/">Seong Hyeon Park</a>, now PhD student at KAIST
  <li><a href="https://www.linkedin.com/in/cmao/">Chengfeng Mao</a>, now PhD student at MIT
  <li><a href="https://www.mit.edu/~ziyinl/">Ziyin Liu</a>, then PhD student at University of Tokyo, now PostDoc at MIT
  <li>Irene Li, now at SoundHound (CRA honorable mention)
</ul>

<br/>
I have an Erd&#337;s number of 3 (Paul Erd&#337;s &rarr; Giuseppe Melfi &rarr; Erik Cambria &rarr; Paul Pu Liang).<br/>
This page has been accessed at least <a href="https://stuff.mit.edu/doc/counter-howto.html"><img src="https://stuff.mit.edu/cgi/counter/pliang" alt="several" style="PADDING-TOP: 20px"></a> times since Feb 8, 2018.

<left>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=100&t=n&d=DOV03OEja9kGRRSbtFesj7DY1F0BQdVjs4Wtk25YT4Q"></script>
</left>
</body>
</font>
</html>
