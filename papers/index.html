<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Modified from: Deepak Pathak, Jon Barron, and Saurabh Gupta. */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 80%;
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  span.highlight {
  background-color: #ffffd0;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Paul Liang, MIT</title>
  <meta name="Paul Liang's Homepage" http-equiv="Content-Type" content="Paul Liang's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
</head>

<body>
<table width="1000" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Paul Pu Liang</pageheading><br>
    <b>email</b>:&nbsp ppliang (at) mit (dot) edu
  </p>

  <tr>
    <td width="33%" valign="top"><img src="images/photo2023_small.jpeg" width="70%" class="center" style="border-radius:15px">
    <p align=center>
    <a href="cv_07_2024.pdf" target="_blank">CV</a> |
    <a href="bio.txt" target="_blank">Bio</a> |
    <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a> <br/>
    <a href="https://github.com/pliang279">Github</a> |
    <a href="https://twitter.com/pliang279">Twitter</a> <br/>
    <p align=center style="margin-top:-8px;" ><a href="https://twitter.com/pliang279?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @pliang279</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

    <a href="papers/research_statement_paul_liang_2024.pdf">Research</a>, <a href="papers/teaching_statement_paul_liang_2024.pdf">teaching</a>, <a href="papers/diversity_statement_paul_liang_2024.pdf">diversity</a> statements<br/>
    </p>

    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am an Assistant Professor at the <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a>, where I direct the Multisensory Intelligence research group.
    </p>
    
    <p>
    In summer 2024, I was a visiting researcher in the <a href="https://simons.berkeley.edu/programs/summer-cluster-ai-psychology-neuroscience">AI, psychology, and neuroscience program</a> at UC Berkeley's Simons Institute for the Theory of Computing.
    Previously, I received my Ph.D. from the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by <a href="https://www.cs.cmu.edu/~morency/">Louis-Philippe Morency</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a>.
    </p>

    <p><b> Prospective students:</b> I am hiring at all levels (post-docs, PhDs, masters, undergrads, and visitors).
    If you want to join MIT as a graduate student, please apply through the programs in <a href="https://www.media.mit.edu/graduate-program/about-media-arts-sciences/">Media Arts & Sciences</a> or <a href="https://www.eecs.mit.edu/academics/graduate-programs/">EECS</a>,
    and mention my name in your application.<br/>
    I am also happy to collaborate and answer questions about my research and MIT academic programs, please send me an email. I especially encourage students from underrepresented groups to reach out.
    </p>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications </sectionheading>(see most updated list on <a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ">Google Scholar</a>)</td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
<br/>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.03418">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/hemm.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2407.03418" id="hemm">
    <heading>HEMM: Holistic Evaluation of Multimodal Foundation Models</heading></a><br>
    Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2024<br>
    </p>

    <div class="paper" id="hemm">
    <a href="https://arxiv.org/abs/2407.03418">arXiv</a> |
    <a href="https://github.com/pliang279/HEMM">code</a> |
    <a href="javascript:toggleblock('hemm_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('hemm')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="hemm_abs">Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today's models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction tuning yield actionable insights for future work in multimodal foundation models.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024hemm,
  title={HEMM: Holistic Evaluation of Multimodal Foundation Models},
  author={Liang, Paul Pu and Goindani, Akshay and Chafekar, Talha and Mathur, Leena and Yu, Haofei and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2407.03418},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2311.09580">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mmoe.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2311.09580" id="mmoe">
    <heading>MMoE: Enhancing Multimodal Language Models with Mixtures of Multimodal Interaction Experts</heading></a><br>
    Haofei Yu, Jason Qi, Lawrence Jang, Ruslan Salakhutdinov, Louis-Philippe Morency, Paul Pu Liang<br>
    EMNLP 2024 <br>
    </p>

    <div class="paper" id="mmoe">
    <a href="https://arxiv.org/abs/2311.09580">arXiv</a> |
    <a href="https://github.com/l-mathur/social-ai">code</a> |
    <a href="javascript:toggleblock('mmoe_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mmoe')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mmoe_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2404.11023">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/social.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2404.11023" id="social">
    <heading>Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions</heading></a><br>
    Leena Mathur, Paul Pu Liang, Louis-Philippe Morency<br>
    EMNLP 2024 <br>
    </p>

    <div class="paper" id="social">
    <a href="https://arxiv.org/abs/2404.11023">arXiv</a> |
    <a href="https://github.com/l-mathur/social-ai">code</a> |
    <a href="javascript:toggleblock('social_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('social')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mosei_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2404.13362">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/amharic.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2404.13362" id="amharic">
    <heading>Semantically Corrected Amharic Automatic Speech Recognition</heading></a><br>
    Samuael Adnew, Paul Pu Liang<br>
    Deep Learning Indaba 2024 <br>
    </p>

    <div class="paper" id="amharic">
    <a href="https://arxiv.org/abs/2404.13362">arXiv</a> |
    <a href="https://github.com/samuael/postprocessed_geez_asr">code</a> |
    <a href="javascript:toggleblock('amharic_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('amharic')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="amharic_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2209.03430">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/survey.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2209.03430" id="survey">
    <heading>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</heading></a><br>
    Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    ACM Computing Surveys 2024, Tutorials at ICML & ICMI 2023, CVPR & NAACL 2022<br>
    </p>

    <div class="paper" id=“survey”>
    <a href="https://arxiv.org/abs/2209.03430">arXiv</a> |
    <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">website</a> |
    <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">videos</a> |
    <a href="javascript:toggleblock('survey_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('survey')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="survey_abs">Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  volume={56},
  number={10},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}
</pre>
    </div>
  </td>
</tr>  



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2404.18976">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/thesis.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2404.18976" id="thesis">
    <heading>Foundations of Multisensory Artificial Intelligence</heading></a><br>
    Paul Pu Liang<br>
    PhD Thesis 2024. Committee: Louis-Philippe Morency, Ruslan Salakhutdinov, Manuel Blum, Lenore Blum, Trevor Darrell<br>
    </p>

    <div class="paper" id=“thesis”>
    <a href="https://arxiv.org/abs/2404.18976">arXiv</a> |
    <a href="javascript:toggleblock('thesis_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('thesis')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="thesis_abs">Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024foundations,
  title={Foundations \& trends in multimodal machine learning: Principles, challenges, and open questions},
  author={Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  journal={ACM Computing Surveys},
  volume={56},
  number={10},
  pages={1--42},
  year={2024},
  publisher={ACM New York, NY}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2311.10227">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/tom.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2311.10227" id="tom">
    <heading>Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</heading></a><br>
    Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency<br>
    ACL 2024 <br>
    </p>

    <div class="paper" id="tom">
    <a href="https://arxiv.org/abs/2311.10227">arXiv</a> |
    <a href="https://github.com/shawnsihyunlee/simulatedtom">code</a> |
    <a href="javascript:toggleblock('tom_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('tom')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="tom_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2304.06819">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/survpath.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2304.06819" id="survpath">
    <heading>Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction</heading></a><br>
    Guillaume Jaume, Anurag Vaidya, Richard Chen, Drew Williamson, Paul Pu Liang, Faisal Mahmood<br>
    CVPR 2024 <br>
    </p>

    <div class="paper" id="survpath">
    <a href="https://arxiv.org/abs/2304.06819">arXiv</a> |
    <a href="https://github.com/mahmoodlab/SurvPath">code</a> |
    <a href="javascript:toggleblock('survpath_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('survpath')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="survpath_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_FLHetBench_Benchmarking_Device_and_State_Heterogeneity_in_Federated_Learning_CVPR_2024_paper.html">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/flhet.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href=" " id="flhet">
    <heading>FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning</heading></a><br>
    Junyuan Zhang, Shuang Zeng, Miao Zhang, Runxi Wang, Feifei Wang, Yuyin Zhou, Paul Pu Liang, Liangqiong Qu<br>
    CVPR 2024 <br>
    </p>

    <div class="paper" id="flhet">
    <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_FLHetBench_Benchmarking_Device_and_State_Heterogeneity_in_Federated_Learning_CVPR_2024_paper.html">arXiv</a> |
    <a href="https://github.com/Carkham/FLHetBench">code</a> |
    <a href="javascript:toggleblock('flhet_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('flhet')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="flhet_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2306.04539">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/dis.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2306.04539" id="dis">
    <heading>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</heading></a><br>
    Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICLR 2024 <br>
    </p>

    <div class="paper" id="dis">
    <a href="https://arxiv.org/abs/2306.04539">arXiv</a> |
    <a href="https://github.com/pliang279/PID">code</a> |
    <a href="javascript:toggleblock('dis_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('dis')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="dis_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.12247">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/pid.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2302.12247" id="pid">
    <heading>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</heading></a><br>
    Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2023<br>
    </p>

    <div class="paper" id="pid">
    <a href="https://arxiv.org/abs/2302.12247">arXiv</a> |
    <a href="https://github.com/pliang279/PID">code</a> |
    <a href="javascript:toggleblock('pid_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('pid')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="pid_abs">The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2024quantifying,
  title={Quantifying \& modeling multimodal interactions: An information decomposition framework},
  author={Liang, Paul Pu and Cheng, Yun and Fan, Xiang and Ling, Chun Kai and Nie, Suzanne and Chen, Richard and Deng, Zihao and Allen, Nicholas and Auerbach, Randy and Mahmood, Faisal and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2306.05268">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/factorcl.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2306.05268" id="factorcl">
    <heading>Factorized Contrastive Learning: Going Beyond Multi-view Redundancy</heading></a><br>
    Paul Pu Liang*, Zihao Deng*, Martin Ma*, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    NeurIPS 2023 <br>
    </p>

    <div class="paper" id="factorcl">
    <a href="https://arxiv.org/abs/2306.05268">arXiv</a> |
    <a href="https://github.com/pliang279/FactorCL">code</a> |
    <a href="javascript:toggleblock('factorcl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('factorcl')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="factorcl_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2312.04837">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mmskd.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2312.04837" id="mmskd">
    <heading>Localized Symbolic Knowledge Distillation for Visual Commonsense Models</heading></a><br>
    Jae Sung Park, Jack Hessel, Khyathi Chandu, Paul Pu Liang, Ximing Lu, Qiuyuan Huang, Peter West, Jianfeng Gao, Ali Farhadi, Yejin Choi<br>
    NeurIPS 2023 <br>
    </p>

    <div class="paper" id="mmskd">
    <a href="https://arxiv.org/abs/2312.04837">arXiv</a> |
    <a href="https://github.com/jamespark3922/localized-skd">code</a> |
    <a href="javascript:toggleblock('mmskd_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mmskd')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mmskd_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2302.04449">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/rnr.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2302.04449" id="rnr">
    <heading>Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals</heading></a><br>
    Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom Mitchell<br>
    NeurIPS 2023, ICLR 2023 Workshop on Reincarnating RL <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="rnr">
    <a href="https://arxiv.org/abs/2302.04449">arXiv</a> |
    <a href="https://github.com/Holmeswww/RnR">code</a> |
    <a href="javascript:toggleblock('rnr_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('rnr')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="rnr_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2305.14577">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/diffm.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2305.14577" id="diffm">
    <heading>Difference-Masking: Choosing What to Mask in Continued Pretraining</heading></a><br>
    Alex Wilf, Syeda Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency<br>
    EMNLP 2023 Findings <br>
    </p>

    <div class="paper" id="diffm">
    <a href="https://arxiv.org/abs/2305.14577">arXiv</a> |
    <a href="https://github.com/abwilf/Difference-Masking">code</a> |
    <a href="javascript:toggleblock('diffm_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('diffm')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="diffm_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2306.04125">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/humanpid.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2306.04125" id="humanpid">
    <heading>Multimodal Fusion Interactions: A Study of Human and Automatic Quantification</heading></a><br>
    Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ICMI 2023 <br>
    </p>

    <div class="paper" id="humanpid">
    <a href="https://arxiv.org/abs/2306.04125">arXiv</a> |
    <a href="https://github.com/pliang279/PID">code</a> |
    <a href="javascript:toggleblock('humanpid_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('humanpid')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="humanpid_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2305.12369">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/hiint.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2305.12369" id="hiint">
    <heading>HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer</heading></a><br>
    Yubin Kim, Dong Won Lee, Paul Pu Liang, Sharifa Algohwinem, Cynthia Breazeal, Hae Won Park<br>
    ICMI 2023 <br>
    </p>

    <div class="paper" id="hiint">
    <a href="https://arxiv.org/abs/2305.12369">arXiv</a> |
    <a href="javascript:toggleblock('hiint_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('hiint')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="hiint_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2208.08080">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lecture.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2208.08080" id="lecture">
    <heading>Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos</heading></a><br>
    Dong Won Lee, Chaitanya Ahuja, Paul Pu Liang, Sanika Natu, Louis-Philippe Morency<br>
    ICCV 2023 <br>
    </p>

    <div class="paper" id="lecture">
    <a href="https://arxiv.org/abs/2208.08080">arXiv</a> |
    <a href="https://github.com/dondongwon/MLPDataset">code</a> |
    <a href="javascript:toggleblock('lecture_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('lecture')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="lecture_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2212.10549">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/cacr.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2212.10549" id="cacr">
    <heading>Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment</heading></a><br>
    Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ACL 2023 <br>
    </p>

    <div class="paper" id="cacr">
    <a href=" ">arXiv</a> |
    <a href="https://github.com/KhoomeiK/CACR">code</a> |
    <a href="javascript:toggleblock('cacr_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('cacr')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="cacr_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2306.04597">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/gender.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2306.04597" id="gender">
    <heading>Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions</heading></a><br>
    Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency<br>
    ACL 2023 <br>
    </p>

    <div class="paper" id="gender">
    <a href="https://arxiv.org/abs/2306.04597">arXiv</a> |
    <a href="https://github.com/himansh005/data_debias">code</a> |
    <a href="javascript:toggleblock('gender_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('gender')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="gender_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2211.05750">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/nano.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2211.05750" id="nano">
    <heading>Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control</heading></a><br>
    Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ACL Findings 2023, NeurIPS 2022 Workshop on Human in the Loop Learning  <font color="#DC143C">(oral, best paper nomination) </font> <br>
    </p>

    <div class="paper" id="nano">
    <a href="https://arxiv.org/abs/2211.05750">arXiv</a> |
    <a href="https://github.com/sfanxiang/Nano">code</a> |
    <a href="javascript:toggleblock('nano_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('nano')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="nano_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



​​<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2207.00056">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/multiviz.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2207.00056" id="multiviz">
    <heading>MultiViz: Towards Visualizing and Understanding Multimodal Models</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICLR 2023, CHI 2023 Late-Breaking Work <br>
    </p>

    <div class="paper" id="multiviz">
    <a href="https://arxiv.org/abs/2207.00056">arXiv</a> |
    <a href="https://github.com/pliang279/MultiViz">code</a> |
    <a href="javascript:toggleblock('multiviz_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('multiviz')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="multiviz_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/abstract/document/10095392/">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/adapter.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://ieeexplore.ieee.org/abstract/document/10095392/" id="adapter">
    <heading>FindAdaptNet: Find and Insert Adapters by Learned Layer Importance</heading></a><br>
    Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Pu Liang, Shinji Watanabe<br>
    ACL 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="adapter">
    <a href="https://ieeexplore.ieee.org/abstract/document/10095392/">arXiv</a> |
    <a href="javascript:toggleblock('adapter_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('adapter')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="adapter_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2208.01036">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/f2f.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href=" " id="f2f">
    <heading>Face-to-Face Contrastive Learning for Social Intelligence Question-Answering</heading></a><br>
    Alex Wilf, Martin Ma, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    FG 2023 <br>
    </p>

    <div class="paper" id="f2f">
    <a href="https://arxiv.org/abs/2208.01036">arXiv</a> |
    <a href="https://github.com/abwilf/Factorized">code</a> |
    <a href="javascript:toggleblock('f2f_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('f2f')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="f2f_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2206.04615">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/bigbench.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2206.04615" id="bigbench">
    <heading>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</heading></a><br>
    442 authors<br>
    TMLR 2023 <br>
    </p>

    <div class="paper" id="bigbench">
    <a href="https://arxiv.org/abs/2206.04615">arXiv</a> |
    <a href="https://github.com/google/BIG-bench">code</a> |
    <a href="javascript:toggleblock('bigbench_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('bigbench')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="bigbench_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2203.01311">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/highmmt.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2203.01311" id="highmmt">
    <heading>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    TMLR 2022<br>
    </p>

    <div class="paper" id="highmmt">
    <a href="https://arxiv.org/abs/2203.01311">arXiv</a> |
    <a href="https://github.com/pliang279/HighMMT">code</a> |
    <a href="javascript:toggleblock('highmmt_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('highmmt')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="highmmt_abs">Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalities {X1,X2} are by measuring how much information can be transferred from X1 to X2, while (2) interaction heterogeneity studies how similarly pairs of modalities {X1,X2}, {X3,X4} interact by measuring how much information can be transferred from fusing {X1,X2} to {X3,X4}. We show the importance of these 2 proposed metrics as a way to automatically prioritize the fusion of modalities that contain unique information or interactions. The result is a single model, HighMMT, that scales up to 10 modalities (text, image, audio, video, sensors, proprioception, speech, time-series, sets, and tables) and 15 tasks from 5 research areas. Not only does HighMMT outperform prior methods on the tradeoff between performance and efficiency, it also demonstrates a crucial scaling behavior: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2022high,
  title={High-modality multimodal transformer: Quantifying modality \& interaction heterogeneity for high-modality representation learning},
  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Tsaw, Jeffrey and Liu, Yudong and Mo, Shentong and Yogatama, Dani and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2203.01311},
  year={2022}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2306.16413">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/multizoo.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2306.16413" id="multizoo">
    <heading>MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    JMLR Open Source Software 2022 <br>
    </p>

    <div class="paper" id="multizoo">
    <a href="https://arxiv.org/abs/2306.16413">arXiv</a> |
    <a href="https://multibench.readthedocs.io/en/latest/">website</a> |
    <a href="https://github.com/pliang279/MultiBench">code</a> |
    <a href="javascript:toggleblock('multizoo_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('multizoo')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="multizoo_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2205.00001">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/brainish.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2205.00001" id="brainish">
    <heading>Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness</heading></a><br>
    Paul Pu Liang<br>
    Association for the Scientific Study of Consciousness 2022, Models of Consciousness 2022 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="brainish">
    <a href="https://arxiv.org/abs/2205.00001">arXiv</a> |
    <a href="javascript:toggleblock('brainish_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('brainish')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="brainish_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2210.04714">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/uq.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2210.04714" id="uq">
    <heading>Uncertainty Quantification with Pre-trained Language Models: A Large-scale Empirical Analysis</heading></a><br>
    Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    EMNLP Findings 2022 <br>
    </p>

    <div class="paper" id="uq">
    <a href="https://arxiv.org/abs/2210.04714">arXiv</a> |
    <a href="https://github.com/xiaoyuxin1002/UQ-PLM">code</a> |
    <a href="javascript:toggleblock('uq_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('uq')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="uq_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2206.11249">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/gem.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2206.11249" id="gem">
    <heading>GEMv2: Multilingual NLG Benchmarking in a Single Line of Code</heading></a><br>
    77 authors<br>
    EMNLP Demo Track 2022 <br>
    </p>

    <div class="paper" id="gem">
    <a href="https://arxiv.org/abs/2206.11249">arXiv</a> |
    <a href="https://gem-benchmark.com">code</a> |
    <a href="javascript:toggleblock('gem_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('gem')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="gem_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2203.11130">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/pacs.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2203.11130" id="pacs">
    <heading>PACS: Physical Audiovisual Commonsense Reasoning</heading></a><br>
    Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ECCV 2022 <br>
    </p>

    <div class="paper" id="pacs">
    <a href="https://arxiv.org/abs/2203.11130">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a> |
    <a href="javascript:toggleblock('pacs_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('pacs')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="pacs_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2203.02013">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/dime.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2203.02013" id="dime">
    <heading>DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations</heading></a><br>
    Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    AIES 2022 <br>
    </p>

    <div class="paper" id="dime">
    <a href="https://arxiv.org/abs/2203.02013">arXiv</a> |
    <a href="https://github.com/lvyiwei1/DIME">code</a> |
    <a href="javascript:toggleblock('dime_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('dime')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="dime_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.06047">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/vitfl.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.06047" id="vitfl">
    <heading>Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning</heading></a><br>
    Liangqiong Qu*, Yuyin Zhou*, Paul Pu Liang*, Yingda Xia, Feifei Wang, Li Fei-Fei, Ehsan Adeli, Daniel Rubin<br>
    CVPR 2022 <br>
    </p>

    <div class="paper" id="vitfl">
    <a href="https://arxiv.org/abs/2106.06047">arXiv</a> |
    <a href="https://github.com/Liangqiong/ViT-FL-main">code</a> |
    <a href="javascript:toggleblock('vitfl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('vitfl')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="vitfl_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="multibench">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/multibench.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2107.07502" id="multibench">
    <heading>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</heading></a><br>
    Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2021 <br>
    </p>

    <div class="paper" id="multibench">
    <a href="https://arxiv.org/abs/2107.07502">arXiv</a> |
    <a href="https://multibench.readthedocs.io/en/latest/">website</a> |
    <a href="https://github.com/pliang279/MultiBench">code</a> |
    <a href="javascript:toggleblock('multibench_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('multibench')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="multibench_abs">Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized code, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2021multibench,
  title={Multibench: Multiscale benchmarks for multimodal representation learning},
  author={Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Wu, Zetian and Cheng, Yun and Wu, Jason and Chen, Leslie and Wu, Peter and Lee, Michelle A and Zhu, Yuke and others},
  journal={Advances in neural information processing systems},
  volume={2021},
  year={2021},
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2101.08919">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/speechprivacy.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2101.08919" id="speechprivacy">
    <heading>Understanding the Tradeoffs in Client-side Privacy for Speech Recognition</heading></a><br>
    Peter Wu, Paul Pu Liang, Jiatong Shi, Ruslan Salakhutdinov, Shinji Watanabe, Louis-Philippe Morency<br>
    Asia Pacific Signal and Information Processing Association Annual Summit and Conference 2021 <br>
    </p>

    <div class="paper" id="speechprivacy">
    <a href="https://arxiv.org/abs/2101.08919">arXiv</a> |
    <a href="https://github.com/peter-yh-wu/speech-privacy">code</a> |
    <a href="javascript:toggleblock('speechprivacy_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('speechprivacy')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="speechprivacy_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.13219">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lmbias.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.13219" id="lmbias">
    <heading>Towards Understanding and Mitigating Social Biases in Language Models</heading></a><br>
    Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICML 2021<br>
    </p>

    <div class="paper" id="lmbias">
    <a href="https://arxiv.org/abs/2106.13219">arXiv</a> |
    <a href="https://github.com/pliang279/LM_bias">code</a> |
    <a href="javascript:toggleblock('lmbias_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('lmbias')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="lmbias_abs">As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{liang2021towards,
  title={Towards understanding and mitigating social biases in language models},
  author={Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={International Conference on Machine Learning},
  pages={6565--6576},
  year={2021},
  organization={PMLR}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2106.13213">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mobile.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2106.13213" id="mobile">
    <heading>Learning Language and Multimodal Privacy Preserving Markers of Mood from Mobile Data</heading></a><br>
    Paul Pu Liang*, Terrance Liu*, Anna Cai, Michal Muszynski, Ryo Ishii, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ACL 2021 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="mobile">
    <a href="https://arxiv.org/abs/2106.13213">arXiv</a> |
    <a href="javascript:toggleblock('mobile_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mobile')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mobile_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2012.02813">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/croma.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2012.02813" id="croma">
    <heading>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</heading></a><br>
    Paul Pu Liang*, Peter Wu*, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ACM Multimedia 2021 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="croma">
    <a href="https://arxiv.org/abs/2012.02813">arXiv</a> |
    <a href="https://github.com/peter-yh-wu/xmodal">code</a> |
    <a href="javascript:toggleblock('croma_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('croma')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="croma_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2104.05196">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/style.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2104.05196" id="style">
    <heading>StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</heading></a><br>
    Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabás Póczos, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NAACL 2021 <br>
    </p>

    <div class="paper" id="style">
    <a href="https://arxiv.org/abs/2104.05196">arXiv</a> |
    <a href="https://github.com/lvyiwei1/StylePTB">code</a> |
    <a href="javascript:toggleblock('style_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('style')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="style_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2003.08197">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/ant.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2003.08197" id="ant">
    <heading>Anchor and Transform: Learning Sparse Embeddings for Large Vocabularies</heading></a><br>
    Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed<br>
    ICLR 2021 <br>
    </p>

    <div class="paper" id="ant">
    <a href="https://arxiv.org/abs/2003.08197">arXiv</a> |
    <a href="https://github.com/pliang279/sparse_discrete">code</a> |
    <a href="javascript:toggleblock('ant_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('ant')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="ant_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://www.aclweb.org/anthology/2020.emnlp-main.141">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/moseas.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.141" id="moseas">
    <heading>MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French</heading></a><br>
    Amir Zadeh, Yansheng Cao, Simon Hessner, Paul Pu Liang, Soujanya Poria, Louis-Philippe Morency<br>
    EMNLP 2020 <br>
    </p>

    <div class="paper" id="moseas">
    <a href="https://www.aclweb.org/anthology/2020.emnlp-main.141">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a> |
    <a href="javascript:toggleblock('moseas_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('moseas')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="moseas_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2003.03212">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/traj.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2003.03212" id="traj">
    <heading>Diverse and Admissible Trajectory Prediction through Multimodal Context Understanding</heading></a><br>
    Seong Hyeon Park, Gyubok Lee, Manoj Bhat, Jimin Seo, Minseok Kang, Jon Francis, Ashwin Jadhav, Paul Pu Liang, Louis-Philippe Morency<br>
    ECCV 2020, CVPR 2020 Argoverse competition <font color="#DC143C">(honorable mention award) </font> <br>
    </p>

    <div class="paper" id="traj">
    <a href="https://arxiv.org/abs/2003.03212">arXiv</a> |
    <a href="https://github.com/kami93/CMU-DATF">code</a> |
    <a href="javascript:toggleblock('traj_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('traj')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="traj_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2007.08100">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/sentbias.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2007.08100" id="sentbias">
    <heading>Towards Debiasing Sentence Representations</heading></a><br>
    Paul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ACL 2020 <br>
    </p>

    <div class="paper" id="sentbias">
    <a href="https://arxiv.org/abs/2007.08100">arXiv</a> |
    <a href="https://github.com/pliang279/sent_debias">code</a> |
    <a href="javascript:toggleblock('sentbias_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('sentbias')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="sentbias_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2003.01848">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/teams.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2003.01848" id="teams">
    <heading>On Emergent Communication in Competitive Multi-Agent Teams</heading></a><br>
    Paul Pu Liang, Jeffrey Chen, Ruslan Salakhutdinov, Louis-Philippe Morency, Satwik Kottur<br>
    AAMAS 2020 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="teams">
    <a href="https://arxiv.org/abs/2003.01848">arXiv</a> |
    <a href="https://github.com/pliang279/Competitive-Emergent-Communication">code</a> |
    <a href="javascript:toggleblock('teams_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('teams')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="teams_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520303006">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/colearning.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520303006" id="mosei">
    <heading>Empirical and Theoretical Studies of Multimodal Co-learning</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Louis-Philippe Morency<br>
    Elsevier Information Fusion 2020 <br>
    </p>

    <div class="paper" id="colearning">
    <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520303006">arXiv</a> |
    <a href="javascript:toggleblock('colearning_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('colearning')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="colearning_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2001.01523">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/fl.png" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/2001.01523" id="fl">
    <heading>Think Locally, Act Globally: Federated Learning with Local and Global Representations</heading></a><br>
    Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award) </font> <br>
    </p>

    <div class="paper" id="fl">
    <a href="https://arxiv.org/abs/2001.01523">arXiv</a> |
    <a href="https://github.com/pliang279/LG-FedAvg">code</a> |
    <a href="javascript:toggleblock('fl_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('fl')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="fl_abs">Federated learning is a method of training models on private data distributed over multiple devices. To keep device data private, the global model is trained by only communicating parameters and updates which poses scalability challenges for large models. To this end, we propose a new federated learning algorithm that jointly learns compact local representations on each device and a global model across all devices. As a result, the global model can be smaller since it only operates on local representations, reducing the number of communicated parameters. Theoretically, we provide a generalization analysis which shows that a combination of local and global models reduces both variance in the data as well as variance across device distributions. Empirically, we demonstrate that local models enable communication-efficient training while retaining performance. We also evaluate on the task of personalized mood prediction from real-world mobile data where privacy is key. Finally, local models handle heterogeneous data from new devices, and learn fair representations that obfuscate protected attributes such as race, age, and gender.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @article{liang2020think,
  title={Think locally, act globally: Federated learning with local and global representations},
  author={Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Allen, Nicholas B and Auerbach, Randy P and Brent, David and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2001.01523},
  year={2020}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1907.00208">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/gamblers.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1907.00208" id="gamblers">
    <heading>Deep Gamblers: Learning to Abstain with Portfolio Theory</heading></a><br>
    Liu Ziyin, Zhikang Wang, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency, Masahito Ueda<br>
    NeurIPS 2019 <br>
    </p>

    <div class="paper" id="gamblers">
    <a href="https://arxiv.org/abs/1907.00208">arXiv</a> |
    <a href="https://github.com/Z-T-WANG/NIPS2019DeepGamblers">code</a> |
    <a href="javascript:toggleblock('gamblers_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('gamblers')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="gamblers_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1907.01011">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/t2fn.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1907.01011" id="t2fn">
    <heading>Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization</heading></a><br>
   Paul Pu Liang*, Zhun Liu*, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    ACL 2019 <br>
    </p>

    <div class="paper" id="t2fn">
    <a href="https://arxiv.org/abs/1907.01011">arXiv</a> |
    <a href="javascript:toggleblock('t2fn_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('t2fn')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="t2fn_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1906.00295">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mult.gif" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1906.00295" id="mult">
    <heading>Multimodal Transformer for Unaligned Multimodal Language Sequences</heading></a><br>
    Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ACL 2019<br>
    </p>

    <div class="paper" id="mult">
    <a href="https://arxiv.org/abs/1906.00295">arXiv</a> |
    <a href="https://github.com/yaohungt/Multimodal-Transformer">code</a> |
    <a href="javascript:toggleblock('mult_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mult')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mult_abs">Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{tsai2019multimodal,
  title={Multimodal Transformer for Unaligned Multimodal Language Sequences},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={6558--6569},
  year={2019}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/socialiq.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf" id="mosei">
    <heading>Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence</heading></a><br>
    Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, Louis-Philippe Morency<br>
    CVPR 2019 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="socialiq">
    <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf">arXiv</a> |
    <a href="https://cmu-multicomp-lab.github.io/social-iq-2.0/">code</a> |
    <a href="javascript:toggleblock('socialiq_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('socialiq')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="socialiq_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1906.02125">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/baselines.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1906.02125" id="baselines">
    <heading>Strong and Simple Baselines for Multimodal Utterance Embeddings</heading></a><br>
    Paul Pu Liang*, Yao Chong Lim*, Yao-Hung Hubert Tsai, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    NAACL 2019 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="baselines">
    <a href="https://arxiv.org/abs/1906.02125">arXiv</a> |
    <a href="https://github.com/yaochie/multimodal-baselines">code</a> |
    <a href="javascript:toggleblock('baselines_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('baselines')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="baselines_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1806.06176">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/factorized.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1806.06176" id="factorized">
    <heading>Learning Factorized Multimodal Representations</heading></a><br>
    Yao-Hung Hubert Tsai*, Paul Pu Liang*, Amir Zadeh, Louis-Philippe Morency, Ruslan Salakhutdinov<br>
    ICLR 2019 <br>
    </p>

    <div class="paper" id="factorized">
    <a href="https://arxiv.org/abs/1806.06176">arXiv</a> |
    <a href="https://github.com/pliang279/factorized">code</a> |
    <a href="javascript:toggleblock('factorized_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('factorized')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="factorized_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1812.07809">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mctn.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1812.07809" id="mctn">
    <heading>Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities</heading></a><br>
    Hai Pham*, Paul Pu Liang*, Thomas Manzini, Louis-Philippe Morency, Barnabas Poczos<br>
    AAAI 2019 <br>
    </p>

    <div class="paper" id="mctn">
    <a href="https://arxiv.org/abs/1812.07809">arXiv</a> |
    <a href="https://github.com/hainow/MCTN">code</a> |
    <a href="javascript:toggleblock('mctn_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mctn')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mctn_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1811.09362">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/raven.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1811.09362/" id="raven">
    <heading>Words can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors</heading></a><br>
    Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    AAAI 2019 <br>
    </p>

    <div class="paper" id="raven">
    <a href="https://arxiv.org/abs/1811.09362">arXiv</a> |
    <a href="https://github.com/victorywys/RAVEN">code</a> |
    <a href="javascript:toggleblock('raven_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('raven')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="raven_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="papers/dap2018_mosei.pdf">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/msthesis.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="papers/dap2018_mosei.pdf" id="msthesis">
    <heading>Computational Modeling of Human Multimodal Language: The MOSEI Dataset and Interpretable Dynamic Fusion</heading></a><br>
    Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br>
    CMU Master's Thesis 2018, CMU Machine Learning Data Analysis Project <font color="#DC143C">(first runner-up award) </font> <br>
    </p>

    <div class="paper" id="msthesis">
    <a href="papers/dap2018_mosei.pdf">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a> |
    <a href="javascript:toggleblock('msthesis_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('msthesis')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="msthesis_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1808.03920">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/rmfn.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1808.03920" id="mosei">
    <heading>Multimodal Language Analysis with Recurrent Multistage Fusion</heading></a><br>
    Paul Pu Liang, Ziyin Liu, Amir Zadeh, Louis-Philippe Morency<br>
    EMNLP 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="rmfn">
    <a href="https://aclanthology.org/P18-1208/">arXiv</a> |
    <a href="javascript:toggleblock('rmfn_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('rmfn')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="rmfn_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1809.04931">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/lg.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1809.04931" id="lg">
    <heading>Multimodal Local-Global Ranking Fusion for Emotion Recognition</heading></a><br>
    Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    ICMI 2018 <br>
    </p>

    <div class="paper" id="lg">
    <a href="https://arxiv.org/abs/1809.04931">arXiv</a> |
    <a href="javascript:toggleblock('lg_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('lg')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="lg_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>




<tr>
  <td width="33%" valign="top" align="center"><a href="https://aclanthology.org/P18-1208/">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mosei.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://aclanthology.org/P18-1208/" id="mosei">
    <heading>Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, Louis-Philippe Morency<br>
    ACL 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="mosei">
    <a href="https://aclanthology.org/P18-1208/">arXiv</a> |
    <a href="https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK">code</a> |
    <a href="javascript:toggleblock('mosei_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('mosei')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mosei_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competitive performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1806.00064">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/tensor.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1806.00064" id="tensor">
    <heading>Efficient Low-rank Multimodal Fusion with Modality-Specific Factors</heading></a><br>
    Zhun Liu, Ying Shen, Varun Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br>
    ACL 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="tensor">
    <a href="https://arxiv.org/abs/1806.00064">arXiv</a> |
    <a href="https://github.com/Justin1904/Low-rank-Multimodal-Fusion">code</a> |
    <a href="javascript:toggleblock('tensor_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('tensor')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="tensor_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1812.07903">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/leverage.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1812.07903" id="leverage">
    <heading>An Empirical Evaluation of Sketched SVD and its Application to Leverage Score Ordering</heading></a><br>
    Hui Han Chin, Paul Pu Liang<br>
    ACML 2018 <br>
    </p>

    <div class="paper" id="leverage">
    <a href="https://arxiv.org/abs/1812.07903">arXiv</a> |
    <a href="javascript:toggleblock('leverage_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('leverage')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="leverage_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1802.00923">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/marn.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1802.00923" id="marn">
    <heading>Multi-attention Recurrent Network for Human Communication Comprehension</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, Louis-Philippe Morency<br>
    AAAI 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="marn">
    <a href="https://arxiv.org/abs/1802.00923">arXiv</a> |
    <a href="https://github.com/A2Zadeh/MARN">code</a> |
    <a href="javascript:toggleblock('marn_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib('marn')" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="marn_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>



<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1802.00927">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/mfn.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1802.00927" id="mfn">
    <heading>Memory Fusion Network for Multi-view Sequential Learning</heading></a><br>
    Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, Louis-Philippe Morency<br>
    AAAI 2018 <font color="#DC143C">(oral) </font> <br>
    </p>

    <div class="paper" id="mfn">
    <a href="https://arxiv.org/abs/1802.00927">arXiv</a> |
    <a href="https://github.com/pliang279/MFN">code</a> |
    <a href="javascript:toggleblock('mfn_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib(‘mfn’)" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="mfn_abs">Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{zadeh2018multimodal,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2236--2246},
  year={2018}
}
</pre>
    </div>
  </td>
</tr>


<tr>
  <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1802.00924">
  <img alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;border:1px solid black" src="images/icmi.jpg" />
  </a></td>
  <td width="67%" valign="top">
    <p><a href="https://arxiv.org/abs/1802.00924" id="icmi">
    <heading>Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning</heading></a><br>
    Minghai Chen*, Sen Wang*, Paul Pu Liang*, Tadas Baltrusaitis, Amir Zadeh, Louis-Philippe Morency<br>
    ICMI 2017 <font color="#DC143C">(oral, best paper honorable mention) </font> <br>
    </p>

    <div class="paper" id="icmi">
    <a href="https://arxiv.org/abs/1802.00924">arXiv</a> |
    <a href="https://drive.google.com/drive/u/0/folders/1NxyFuogyzNFoCH0Zi5aIXUGYKGuSY9TY">code</a> |
    <a href="javascript:toggleblock('icmi_abs')">abstract</a> |
    <a shape="rect" href="javascript:togglebib(‘icmi’)" class="togglebib">bibtex</a>

    <p align="justify"> <i style="display: none;" id="icmi_abs">With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we propose a novel deep architecture for multimodal sentiment analysis that is able to perform modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding allows us to alleviate the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention can perform word level fusion at a finer fusion resolution between the input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. These results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.
    </i></p>

<pre xml:space="preserve" style="display:none;">
  @inproceedings{chen2017multimodal,
  title={Multimodal sentiment analysis with word-level fusion and reinforcement learning},
  author={Chen, Minghai and Wang, Sen and Liang, Paul Pu and Baltru{\v{s}}aitis, Tadas and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the 19th ACM international conference on multimodal interaction},
  pages={163--171},
  year={2017}
}
</pre>
    </div>
  </td>
</tr>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Modified version of template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>

</body>

</html>
