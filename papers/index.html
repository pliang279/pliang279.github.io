<!DOCTYPE html>
<html>
<font face="Verdana" size = "2.5">
<head>

<style type="text/css">
A:link { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:visited { COLOR: #0903A5; TEXT-DECORATION: none; font-weight: normal }
A:active { COLOR: #0903A5; TEXT-DECORATION: none }
A:hover { COLOR: #C80537; TEXT-DECORATION: none; font-weight: none }
img {
  margin-top: -20px;
  margin-bottom: 0px;
}
h1 {
  margin-top: 15px;
  margin-bottom: 15px;
}
h2 {
  margin-top: 10px;
  margin-bottom: 10px;
}
h2 + ul {
  margin-top: -5px;
  margin-bottom: 0px;
}
h2 + ol {
  margin-top: 0px;
  margin-bottom: 20px;
}
h3 + ol {
  margin-top: -10px;
  margin-bottom: -10px;
}
hr {
    display: block;
    height: 1px;
    border: 0;
    border-top: 1px solid #ccc;
    margin: 1em 0;
    padding: 0;
}
</style>

<title>Paul Liang, MIT</title>

</head>

<body>

<img src="../images/photo2023_small.jpeg" width="170" height="170" style="float: left; PADDING-TOP: 20px; PADDING-RIGHT: 10px; PADDING-BOTTOM: 0px"/>

<h1>Paul Pu Liang</h1>
Assistant Professor, <a href="https://www.media.mit.edu/">MIT Media Lab</a> and <a href="https://www.eecs.mit.edu/">MIT EECS</a><br/>
Email: ppliang(at)mit.edu<br/>
<br/>
<br/>

<a href="../cv_07_2024.pdf"><font size="+1.5">[CV]</font></a>

<a href="https://scholar.google.com/citations?hl=en&user=pKf5LtQAAAAJ"> <img src="../images/gscholar.jpg" width="24" height="24" style="margin-bottom:-7px;PADDING-LEFT:5px;PADDING-RIGHT:5px"></a>

<a href="https://www.linkedin.com/in/paulpuliang/"> <img src="../images/linkedin.png" width="20" height="20" style="margin-bottom:-5px;PADDING-LEFT:5px;PADDING-RIGHT:5px"></a> 

<a href="https://github.com/pliang279"> <img src="../images/github.png" width="30" height="30" style="margin-bottom:-10px">@pliang279</a>

<a href="https://twitter.com/pliang279/"> <img src="../images/twitter.png" width="30" height="30" style="margin-bottom:-10px">@pliang279</a>

<a href="https://www.instagram.com/lpwinniethepu/"> <img src="../images/instagram.png" width="22" height="22" style="margin-bottom:-6px;PADDING-LEFT:5px;PADDING-TOP: 10px"> @lpwinniethepu</a>
<br/>
<br/>

<h2 id="Publications">Publications</h2>

(* denotes joint first-authors)

<h3>2024</h3>

<ul style="line-height:140%">

  <li><b>Foundations of Multisensory Artificial Intelligence</b></li>
  Paul Pu Liang<br/>
  PhD Thesis. Committee: Louis-Philippe Morency, Ruslan Salakhutdinov, Manuel Blum, Lenore Blum, Trevor Darrell<br/>
  <a href="https://arxiv.org/abs/2404.18976">[arXiv]</a>

  <li><b>HEMM: Holistic Evaluation of Multimodal Foundation Models</b></li>
  Paul Pu Liang, Akshay Goindani, Talha Chafekar, Leena Mathur, Haofei Yu, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  <a href="https://arxiv.org/abs/2407.03418">[arXiv]</a> <a href="https://github.com/pliang279/HEMM">[code]</a>

  <li><b>IoT-LM: Large Multisensory Language Models for the Internet of Things</b></li>
  Shentong Mo, Ruslan Salakhutdinov, Louis-Philippe Morency, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2407.09801">[arXiv]</a> <a href="https://github.com/Multi-IoT/MultiIoT">[code]</a>

  <li><b>MMoE: Enhancing Multimodal Language Models with Mixtures of Multimodal Interaction Experts</b></li>
  Haofei Yu, Jason Qi, Lawrence Jang, Ruslan Salakhutdinov, Louis-Philippe Morency, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2311.09580">[arXiv]</a>

  <li><b>Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions</b></li>
  Leena Mathur, Paul Pu Liang, Louis-Philippe Morency<br/>
  <a href="https://arxiv.org/abs/2404.11023">[arXiv]</a> <a href="https://github.com/l-mathur/social-ai">[code]</a>

  <li><b>Semantically Corrected Amharic Automatic Speech Recognition</b></li>
  Samuael Adnew, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2404.13362">[arXiv]</a> <a href="https://github.com/samuael/postprocessed_geez_asr">[code]</a>

  <li><b>MultiIoT: Benchmarking Machine Learning for the Internet of Things</b></li>
  Shentong Mo, Louis-Philippe Morency, Ruslan Salakhutdinov, Paul Pu Liang<br/>
  <a href="https://arxiv.org/abs/2311.06217">[arXiv]</a> <a href="https://github.com/Multi-IoT/MultiIoT">[code]</a>

  <li><b>Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions</b></li>
  Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  ACM Computing Surveys, Tutorials at ICML 2023, ICMI 2023, CVPR 2022, NAACL 2022<br/>
  <a href="https://arxiv.org/abs/2209.03430">[arXiv]</a> <a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">[tutorial website]</a> <a href="https://www.youtube.com/playlist?list=PLki3HkfgNEsKPcpj5Vv2P98SRAT9wxIDa">[tutorial videos]</a>

  <li><b>Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</b></li>
  Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency<br/>
  ACL 2024<br/>
  <a href="https://arxiv.org/abs/2311.10227">[arXiv]</a> <a href="https://github.com/shawnsihyunlee/simulatedtom">[code]</a>
  
  <li><b>Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction</b></li>
  Guillaume Jaume, Anurag Vaidya, Richard Chen, Drew Williamson, Paul Pu Liang, Faisal Mahmood<br/>
  CVPR 2024<br/>
  <a href="https://arxiv.org/abs/2304.06819">[arXiv]</a> <a href="https://github.com/mahmoodlab/SurvPath">[code]</a>

  <li><b>FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning</b></li>
  Junyuan Zhang, Shuang Zeng, Miao Zhang, Runxi Wang, Feifei Wang, Yuyin Zhou, Paul Pu Liang, Liangqiong Qu<br/>
  CVPR 2024<br/>
  <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_FLHetBench_Benchmarking_Device_and_State_Heterogeneity_in_Federated_Learning_CVPR_2024_paper.html">[arXiv]</a>
  
  <li><b>Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications</b></li>
  Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICLR 2024<br/>
  <a href="https://arxiv.org/abs/2306.04539">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>
</ul>


<h3>2023</h3>

<ul style="line-height:140%">

  <li><b>Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework</b></li>
  Paul Pu Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard Chen, Zihao Deng, Nicholas Allen, Randy Auerbach, Faisal Mahmood, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2023<br/>
  <a href="https://arxiv.org/abs/2302.12247">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>Factorized Contrastive Learning: Going Beyond Multi-view Redundancy</b></li>
  Paul Pu Liang*, Zihao Deng*, Martin Ma*, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  NeurIPS 2023<br/>
  <a href="https://arxiv.org/abs/2306.05268">[arXiv]</a> <a href="https://github.com/pliang279/FactorCL">[code]</a>

  <li><b>Localized Symbolic Knowledge Distillation for Visual Commonsense Models</b></li>
  Jae Sung Park, Jack Hessel, Khyathi Chandu, Paul Pu Liang, Ximing Lu, Qiuyuan Huang, Peter West, Jianfeng Gao, Ali Farhadi, Yejin Choi<br/>
  NeurIPS 2023<br/>
  <a href="https://arxiv.org/abs/2312.04837">[arXiv]</a> <a href="https://github.com/jamespark3922/lskd">[code]</a>

  <li><b>Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals</b></li>
  Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom Mitchell<br/>
  NeurIPS 2023, ICLR 2023 Workshop on Reincarnating RL <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2302.04449">[arXiv]</a> <a href="https://github.com/Holmeswww/RnR">[code]</a>

  <li><b>Difference-Masking: Choosing What to Mask in Continued Pretraining</b></li>
  Alex Wilf, Syeda Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency<br/>
  EMNLP 2023 Findings<br/>
  <a href="https://arxiv.org/abs/2305.14577">[arXiv]</a>

  <li><b>Multimodal Fusion Interactions: A Study of Human and Automatic Quantification</b></li>
  Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ICMI 2023<br/>
  <a href="https://arxiv.org/abs/2306.04125">[arXiv]</a> <a href="https://github.com/pliang279/PID">[code]</a>

  <li><b>HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer</b></li>
  Yubin Kim, Dong Won Lee, Paul Pu Liang, Sharifa Algohwinem, Cynthia Breazeal, Hae Won Park<br/>
  ICMI 2023<br/>
  <a href="https://arxiv.org/abs/2305.12369">[arXiv]</a>

  <li><b>Multimodal Lecture Presentations Dataset: Understanding Multimodality in Educational Slides</b></li>
  Dong Won Lee, Chaitanya Ahuja, Paul Pu Liang, Sanika Natu, Louis-Philippe Morency<br/>
  ICCV 2023<br/>
  <a href="https://arxiv.org/abs/2208.08080">[arXiv]</a> <a href="https://github.com/dondongwon/MLPDataset">[code]</a>

  <li><b>Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment</b></li>
  Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2023<br/>
  <a href="https://arxiv.org/abs/2212.10549">[arXiv]</a> <a href="https://github.com/KhoomeiK/CACR">[code]</a>

  <li><b>Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions</b></li>
  Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency<br/>
  ACL 2023<br/>
  <a href="https://arxiv.org/abs/2306.04597">[arXiv]</a> <a href="https://github.com/himansh005/data_debias">[code]</a>

  <li><b>Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control</b></li>
  Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL Findings 2023, NeurIPS 2022 Workshop on Human in the Loop Learning <font color="#DC143C">(oral, best paper nomination)</font><br/>
  <a href="https://arxiv.org/abs/2211.05750">[arXiv]</a> <a href="https://github.com/sfanxiang/Nano">[code]</a>

  <li><b>MultiViz: Towards Visualizing and Understanding Multimodal Models</b></li>
  Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICLR 2023, CHI 2023 Late Breaking Work<br/>
  <a href="https://arxiv.org/abs/2207.00056">[arXiv]</a> <a href="https://github.com/pliang279/MultiViz">[code]</a>

  <li><b>FindAdaptNet: Find and Insert Adapters by Learned Layer Importance</b></li>
  Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Pu Liang, Shinji Watanabe<br/>
  ICASSP 2023<br/>
  <a href="https://ieeexplore.ieee.org/abstract/document/10095392">[paper]</a>

  <li><b>Face-to-Face Contrastive Learning for Social Intelligence Question-Answering</b></li>
  Alex Wilf, Martin Ma, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  FG 2023<br/>
  <a href="https://arxiv.org/abs/2208.01036">[arXiv]</a> <a href="https://github.com/abwilf/Factorized">[code]</a>

  <li><b>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</b></li>
  442 authors including Paul Pu Liang<br/>
  TMLR 2023<br/>
  <a href="https://arxiv.org/abs/2206.04615">[arXiv]</a> <a href="https://github.com/google/BIG-bench">[code]</a>
</ul>


<h3>2022</h3>

<ul style="line-height:140%">

  <li><b>High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw, Yudong Liu, Dani Yogatama, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  TMLR 2022<br/>
  <a href="https://arxiv.org/abs/2203.01311">[arXiv]</a> <a href="https://github.com/pliang279/HighMMT">[code]</a>

  <li><b>MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  JMLR Open Source Software 2022<br/>
  <a href="https://multibench.readthedocs.io/en/latest/">[website]</a> <a href="https://github.com/pliang279/MultiBench">[code]</a>

  <li><b>Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness</b></li>
  Paul Pu Liang<br/>
  Association for the Scientific Study of Consciousness 2022, Models of Consciousness 2022 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2205.00001">[arXiv]</a>

  <li><b>Uncertainty Quantification with Pre-trained Language Models: A Large-scale Empirical Analysis</b></li>
  Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  EMNLP Findings 2022<br/>
  <a href="https://arxiv.org/abs/2210.04714">[arXiv]</a> <a href="https://github.com/xiaoyuxin1002/UQ-PLM">[code]</a>

  <li><b>GEMv2: Multilingual NLG Benchmarking in a Single Line of Code</b></li>
  77 authors including Paul Pu Liang<br/>
  EMNLP Demo Track 2022<br/>
  <a href="https://arxiv.org/abs/2206.11249">[arXiv]</a> <a href="https://gem-benchmark.com/">[code]</a>

  <li><b>PACS: A Dataset for Physical Audiovisual Commonsense Reasoning</b></li>
  Samuel Yu, Peter Wu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ECCV 2022<br/>
  <a href="https://arxiv.org/abs/2203.11130">[arXiv]</a> <a href="https://github.com/samuelyu2002/PACS">[code]</a>

  <li><b>DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations</b></li>
  Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  AIES 2022<br/>
  <a href="https://arxiv.org/abs/2203.02013">[arXiv]</a> <a href="https://github.com/lvyiwei1/DIME">[code]</a>

  <li><b>Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning</b></li>
  Liangqiong Qu*, Yuyin Zhou*, Paul Pu Liang*, Yingda Xia, Feifei Wang, Li Fei-Fei, Ehsan Adeli, Daniel Rubin<br/>
  CVPR 2022<br/>
  <a href="https://arxiv.org/abs/2106.06047">[arXiv]</a> <a href="https://github.com/Liangqiong/ViT-FL-main">[code]</a>
</ul>


<h3>2021</h3>

<ul style="line-height:140%">

  <li><b>MultiBench: Multiscale Benchmarks for Multimodal Representation Learning</b></li>
  Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michelle Lee, Yuke Zhu, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2021<br/>
  <a href="https://arxiv.org/abs/2107.07502">[arXiv]</a> <a href="https://cmu-multicomp-lab.github.io/multibench/">[website]</a> <a href="https://github.com/pliang279/MultiBench">[code]</a>

  <li><b>Understanding the Tradeoffs in Client-side Privacy for Downstream Speech Tasks</b></li>
  Peter Wu, Paul Pu Liang, Jiatong Shi, Ruslan Salakhutdinov, Shinji Watanabe, Louis-Philippe Morency<br/>
  Asia Pacific Signal and Information Processing Association Annual Summit and Conference 2021</br> 
  <a href="https://arxiv.org/abs/2101.08919">[arXiv]</a> <a href="https://github.com/peter-yh-wu/speech-privacy">[code]</a>

  <li><b>Towards Understanding and Mitigating Social Biases in Language Models</b></li>
  Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICML 2021<br/>
  <a href="https://arxiv.org/abs/2106.13219">[arXiv]</a> <a href="https://github.com/pliang279/LM_bias">[code]</a>

  <li><b>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</b></li>
  Paul Pu Liang*, Terrance Liu*, Anna Cai, Michal Muszynski, Ryo Ishii, Nick Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2021 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2106.13213">[arXiv]</a>

  <li><b>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</b></li>
  Paul Pu Liang*, Peter Wu*, Liu Ziyin, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ACM Multimedia 2021 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2012.02813">[arXiv]</a> <a href="https://github.com/peter-yh-wu/xmodal">[code]</a>

  <li><b>StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer</b></li>
  Yiwei Lyu*, Paul Pu Liang*, Hai Pham*, Eduard Hovy, Barnab&aacute;s P&oacute;czos, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NAACL 2021<br/>
  <a href="https://arxiv.org/abs/2104.05196">[arXiv]</a> <a href="https://github.com/lvyiwei1/StylePTB">[code]</a>

  <li><b>Ask & Explore: Grounded Question Answering for Curiosity-Driven Exploration</b></li>
  Jivat Neet, Yiding Jiang, Paul Pu Liang<br/>
  ICLR 2021 Workshop on Embodied Multimodal Learning<br/>
  <a href="https://arxiv.org/abs/2104.11902">[arXiv]</a>

  <li><b>Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies</b></li>
  Paul Pu Liang, Manzil Zaheer, Yuan Wang, Amr Ahmed<br/>
  ICLR 2021<br/>
  <a href="https://arxiv.org/abs/2003.08197">[arXiv]</a> <a href="https://github.com/pliang279/sparse_discrete">[code]</a>
</ul>


<h3>2020</h3>

<ul style="line-height:140%">

  <li><b>MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French</b></li>
  Amir Zadeh, Yansheng Cao, Simon Hessner, Paul Pu Liang, Soujanya Poria, Louis-Philippe Morency<br/>
  EMNLP 2020<br/>
  <a href="https://www.aclweb.org/anthology/2020.emnlp-main.141.pdf">[paper]</a>

  <li><b>Diverse and Admissible Trajectory Prediction through Multimodal Context Understanding</b></li>
  Seong Hyeon Park, Gyubok Lee, Manoj Bhat, Jimin Seo, Minseok Kang, Jonathan Francis, Ashwin R. Jadhav, Paul Pu Liang, Louis-Philippe Morency<br/>
  ECCV 2020, CVPR 2020 Argoverse competition <font color="#DC143C">(honorable mention award)</font><br/>
  <a href="https://arxiv.org/abs/2003.03212">[arXiv]</a> <a href="https://github.com/kami93/CMU-DATF">[code]</a>

  <li><b>Towards Debiasing Sentence Representations</b></li>
  Paul Pu Liang, Irene Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2020<br/>
  <a href="https://arxiv.org/abs/2007.08100">[arXiv]</a> <a href="https://github.com/pliang279/sent_debias">[code]</a>

  <li><b>On Emergent Communication in Competitive Multi-Agent Teams</b></li>
  Paul Pu Liang, Jeffrey Chen, Ruslan Salakhutdinov, Louis-Philippe Morency, Satwik Kottur<br/>
  AAMAS 2020 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/2003.01848">[arXiv]</a> <a href="https://github.com/pliang279/Competitive-Emergent-Communication">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/aamas2020_communication_slides.pdf">[slides]</a>

  <li><b>Empirical and Theoretical Studies of Multimodal Co-learning</b></li>
  Amir Zadeh, Paul Pu Liang, Louis-Philippe Morency<br/>
  Elsevier Information Fusion 2020<br/>
  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253520303006">[arXiv]</a>
</ul>

<h3>2019</h3>

<ul style="line-height:140%">

  <li><b>Think Locally, Act Globally: Federated Learning with Local and Global Representations</b></li>
  Paul Pu Liang*, Terrance Liu*, Liu Ziyin, Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NeurIPS 2019 Workshop on Federated Learning <font color="#DC143C">(oral, distinguished student paper award)</font><br/>
  <a href="https://arxiv.org/abs/2001.01523">[arXiv]</a> <a href="https://github.com/pliang279/LG-FedAvg">[code]</a>

  <li><b>Deep Gamblers: Learning to Abstain with Portfolio Theory</b></li>
  Liu Ziyin, Zhikang Wang, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency, Masahito Ueda<br/>
  NeurIPS 2019<br/>
  <a href="https://arxiv.org/abs/1907.00208">[arXiv]</a> <a href="https://github.com/Z-T-WANG/NIPS2019DeepGamblers">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/neurips2019_gamblers_poster.pdf">[poster]</a>

  <li><b>Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization</b></li>
  Paul Pu Liang*, Zhun Liu*, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  ACL 2019<br/>
  <a href="https://arxiv.org/abs/1907.01011">[arXiv]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/acl2019_tensor_poster.pdf">[poster]</a>

  <li><b>Multimodal Transformer for Unaligned Multimodal Language Sequences</b></li>
  Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ACL 2019<br/>
  <a href="https://arxiv.org/abs/1906.00295">[arXiv]</a> <a href="https://github.com/yaohungt/Multimodal-Transformer">[code]</a>

  <li><b>Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence</b></li>
  Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund Tong, Louis-Philippe Morency<br/>
  CVPR 2019 <font color="#DC143C">(oral)</font><br/>
  <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf">[paper]</a> <a href="https://www.thesocialiq.com">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/cvpr2019_socialiq_poster.pdf">[poster]</a>

  <li><b>Strong and Simple Baselines for Multimodal Utterance Embeddings</b></li>
  Paul Pu Liang*, Yao Chong Lim*, Yao-Hung Hubert Tsai, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  NAACL 2019 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/1906.02125">[arXiv]</a> <a href="https://github.com/yaochie/multimodal-baselines">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/naacl2019_baselines_slides.pdf">[slides]</a>

  <li><b>Learning Factorized Multimodal Representations</b></li>
  Yao-Hung Hubert Tsai*, Paul Pu Liang*, Amir Zadeh, Louis-Philippe Morency, Ruslan Salakhutdinov<br/>
  ICLR 2019<br/>
  <a href="https://arxiv.org/abs/1806.06176">[arXiv]</a> <a href="https://github.com/pliang279/factorized">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/iclr2019_factorized_poster.pdf">[poster]</a>

  <li> <b>Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities</b></li>
  Hai Pham*, Paul Pu Liang*, Thomas Manzini, Louis-Philippe Morency, Barnab&aacute;s P&oacute;czos<br/>
  AAAI 2019<br/>
  <a href="https://arxiv.org/abs/1812.07809">[arXiv]</a> <a href="https://github.com/hainow/MCTN">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/aaai2019_translations_slides.pdf">[slides]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/aaai2019_translations_poster.pdf">[poster]</a>

  <li><b>Words can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors</b></li>
  Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  AAAI 2019<br/>
  <a href="https://arxiv.org/abs/1811.09362">[arXiv]</a> <a href="https://github.com/victorywys/RAVEN">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/aaai2019_variations_slides.pdf">[slides]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/aaai2019_variations_poster.pdf">[poster]</a>
</ul>

<h3>2018</h3>

<ul style="line-height:140%">

  <li><b>Computational Modeling of Human Multimodal Language: The MOSEI Dataset and Interpretable Dynamic Fusion</b></li>
  Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency<br/>
  Master's Thesis, CMU Machine Learning Data Analysis Project 2018 <font color="#DC143C">(first runner-up award)</font><br/>
  <a href="dap2018_mosei.pdf">[paper]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/dap2018_mosei_slides.pdf">[slides]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/dap2018_mosei_poster.pdf">[poster]</a>

  <li><b>Multimodal Language Analysis with Recurrent Multistage Fusion</b></li>
  Paul Pu Liang, Ziyin Liu, Amir Zadeh, Louis-Philippe Morency<br/>
  EMNLP 2018 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/1808.03920">[arXiv]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/emnlp2018_multistage_slides.pdf">[slides]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/nips2018ws_multistage_poster.pdf">[poster]</a>
  
  <li><b>Multimodal Local-Global Ranking Fusion for Emotion Recognition</b></li>
  Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  <font color="black">ICMI 2018</font><br/>
  <a href="https://arxiv.org/abs/1809.04931">[arXiv]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/icmi2018_ranking_poster.pdf">[poster]</a>
  
  <li><b>Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</b></li>
  Amir Zadeh, Paul Pu Liang, Jonathan Vanbriesen, Soujanya Poria, Edmund Tong, Erik Cambria, Minghai Chen, Louis-Philippe Morency<br/>
  ACL 2018 <font color="#DC143C">(oral)</font><br/>
  <a href="https://aclweb.org/anthology/P18-1208">[arXiv]</a> <a href="https://github.com/A2Zadeh/CMU-MultimodalSDK">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/acl2018_mosei_slides.pdf">[slides]</a>
  
  <li><b>Efficient Low-rank Multimodal Fusion with Modality-Specific Factors</b></li>
  Zhun Liu, Ying Shen, Varun Lakshminarasimhan, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency<br/>
  ACL 2018 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/1806.00064">[arXiv]</a> <a href="https://github.com/Justin1904/Low-rank-Multimodal-Fusion">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/acl2018_lowrank_slides.pdf">[slides]</a>
  
  <li><b>An Empirical Evaluation of Sketched SVD and its Application to Leverage Score Ordering</b></li>
  Hui Han Chin, Paul Pu Liang<br/>
  ACML 2018<br/>
  <a href="https://arxiv.org/abs/1812.07903">[arXiv]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/acml2018_sketching_slides.pdf">[slides]</a> <a href="https://www.cs.cmu.edu/~pliang/posters/acml2018_sketching_poster.pdf">[poster]</a>
  
  <li><b>Multi-attention Recurrent Network for Human Communication Comprehension</b></li>
  Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, Louis-Philippe Morency<br/>
  AAAI 2018 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/1802.00923">[arXiv]</a> <a href="https://github.com/A2Zadeh/MARN">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/aaai2018_marn_slides.pdf">[slides]</a>
  
  <li><b>Memory Fusion Network for Multi-view Sequential Learning</b></li>
  Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, Louis-Philippe Morency<br/>
  AAAI 2018 <font color="#DC143C">(oral)</font><br/>
  <a href="https://arxiv.org/abs/1802.00927">[arXiv]</a> <a href="https://github.com/pliang279/MFN">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/aaai2018_mfn_slides.pdf">[slides]</a>
</ul>

<h3>2017</h3>

<ul style="line-height:140%">

  <li><b>Multimodal Sentiment Analysis with Word-level Fusion and Reinforcement Learning</b></li>
  Minghai Chen*, Sen Wang*, Paul Pu Liang*, Tadas Baltru&scaron;aitis, Amir Zadeh, Louis-Philippe Morency<br/>
  ICMI 2017 <font color="#DC143C">(oral, honorable mention award)</font><br/>
  <a href="https://arxiv.org/abs/1802.00924">[arXiv]</a> <a href="https://drive.google.com/drive/u/0/folders/1NxyFuogyzNFoCH0Zi5aIXUGYKGuSY9TY">[code]</a> <a href="https://www.cs.cmu.edu/~pliang/slides/icmi2017_gme_slides.pdf">[slides]</a>
</ul>


</body>
</font>
</html>
